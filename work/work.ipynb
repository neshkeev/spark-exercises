{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8d14b37-b08b-472d-a0b0-1a6feee00f5e",
   "metadata": {},
   "source": [
    "# Spark Advanced - Bucketing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6112cf4-a31c-49cf-9f98-f9d4f1ca9153",
   "metadata": {},
   "source": [
    "## Мотивация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e6201c-9389-4929-b71b-54a2f2b25ff8",
   "metadata": {},
   "source": [
    "Для выполнения распределенных операций JOIN необходимо запустить Shuffle операцию, которая положит на один и тот же воркер строки с одинаковым ключом. Выполнение перемешивания (shuffle) является основным препятствием на пути повышения производительности. Логически можно предположить, что необходимо заранее положить однаковые строки рядом, чтобы избежать этапа перемешивания. Второй тяжелой операцией во время соединения таблиц является сортировка. Поэтому было бы логично хранить строки в отсортированном порядке. Бакетинг призван решить эти вопросы: пользователь тратит немного больше времени на организацию хранилища, но при этом получает прирост производительности запросов на чтение."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f986ba0-da21-43f4-bdfe-e161dfa555c8",
   "metadata": {},
   "source": [
    "На картинке ниже представлена схема из пяти бакетов:\n",
    "\n",
    "![](../imgs/spark-buckets.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c6c84a-b58f-4989-89ff-a5ae1415738a",
   "metadata": {},
   "source": [
    "## Запуск приложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad78f7c-69a4-44dd-a9df-0a127c8ce423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col, sum, hash, expr\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad143f9f-f53e-4010-926a-21ef7e7ebfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .appName(\"bucketing\")\n",
    "        .master(\"local[4]\")\n",
    "        .config(\"spark.sql.sources.bucketing.enabled\", True)\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "        .config(\"spark.sql.warehouse.dir\", \"/tmp/spark-warehouse\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fca75e-a77c-4213-9a38-cf6ff0cee147",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55328726-8911-4b2e-88dc-43b3e5f46f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd /tmp && rm -rf steam && rm -rf /tmp/spark-warehouse && unzip ~/work/data/steam.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65da8a39-5812-4df2-9007-9d94ce73b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_parquet(df: DataFrame, num: int, loc: str):\n",
    "    (df\n",
    "        .repartition(num)\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .parquet(loc)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee576bfc-5bfe-4fbd-848b-3fd7623dff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Разбить датафреймы steam на 4 партиции\")\n",
    "\n",
    "dump_parquet(spark.read.parquet(\"/tmp/steam/details.parquet\"), 4, \"/tmp/steam_partitions/details\")\n",
    "dump_parquet(spark.read.parquet(\"/tmp/steam/tags.parquet\"), 4, \"/tmp/steam_partitions/tags\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06586ec8-0bd4-4cd8-b23f-f3bfbacea3b9",
   "metadata": {},
   "source": [
    "В датафрейме `games` поле `app_id` имеет тип `int`, а в датафрейме `details` поле `app_id` имеет тип `long`, поэтому необходимо изменить тип колонки `app_id` в `games`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221d819c-203e-462d-b625-15a5f3d25540",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Поменять тип app_id с `int` на `long` в `games`\")\n",
    "\n",
    "df = (\n",
    "  spark\n",
    "    .read\n",
    "    .parquet(\"/tmp/steam/games.parquet\")\n",
    "    .withColumn(\"app_id\", col(\"app_id\").cast(\"long\"))\n",
    ")\n",
    "dump_parquet(df, 4, \"/tmp/steam_partitions/games\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a72efa9-6525-45f1-9253-b5ba8aee3dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Загрузка датафреймов steam\")\n",
    "\n",
    "games_df = spark.read.parquet(\"/tmp/steam_partitions/games\")\n",
    "details_df = spark.read.parquet(\"/tmp/steam_partitions/details\")\n",
    "tags_df = spark.read.parquet(\"/tmp/steam_partitions/tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b6a9aa-273b-46a0-94b4-5b02baa340c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd /tmp && rm -rf taxi.parquet && rm -rf /tmp/spark-warehouse && unzip ~/work/data/taxi.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1a76c5-53e3-44db-8fa1-d4df8ba6a01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Разбить датафрейм taxi на 4 партиции\")\n",
    "\n",
    "dump_parquet(spark.read.parquet(\"/tmp/taxi.parquet\"), 4, \"/tmp/taxi_partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1def135e-6cbe-4123-9976-9a4f02737da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Загрузка датафреймов taxi\")\n",
    "\n",
    "taxi_df = spark.read.parquet(\"/tmp/taxi_partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4167347-fd2e-4cd7-90cc-ef05ac01e444",
   "metadata": {},
   "source": [
    "## Подготовка bucketing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c319f9a4-c85c-4ccc-bb93-cc1d1914ee2e",
   "metadata": {},
   "source": [
    "Только таблицы могут быть бакетированы, а информация о бакетах хранится в каталоге кластера. При работе с таблицами Catalyst может использовать информацию о бакетах, чтобы отсекать ненужные для сканирования данные.\n",
    "\n",
    "> **Только таблицы знают о бакетах, датафреймам эта информация недоступна**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242d1f14-4c11-4a2c-85a2-03c42d86b001",
   "metadata": {},
   "source": [
    "Функция `bucketing` позволит создать таблицу, записи которой будут разложены по бакетам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffce4f7-0130-4aed-9775-fbad4f078db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucketing(df: DataFrame, table_name: str, column: str, buckets: int = 100) -> DataFrame:\n",
    "    (\n",
    "    df.write\n",
    "      .mode(\"overwrite\")\n",
    "      .bucketBy(buckets, column)\n",
    "      .sortBy(column)\n",
    "      .saveAsTable(table_name)\n",
    "    )\n",
    "    return spark.table(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2913427e-60f0-4095-b62f-661595076c3e",
   "metadata": {},
   "source": [
    "**ВАЖНО:** Функция `bucketing` возвращает датафрейм, который основан на таблице. Работа с датафреймом на базе таблицы сделает информацию о бакетах нижележащей таблице доступной оптимизатору Catalyst."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b8dde4-1171-4fac-9dcd-3c2e3af6b940",
   "metadata": {},
   "source": [
    "На картинке ниже представлена логика работы функции `bucketing`:\n",
    "\n",
    "- датафрейм `my_df` из двух партиций превращется в таблицу `my_table`,\n",
    "- таблица `my_table` состоит из пяти бакетов,\n",
    "- в одном бакете два файла: по одному на каждую партицию датафрейма `my_df`,\n",
    "- в каждом файле бакета присутствует локальная сортировка.\n",
    "\n",
    "![](../imgs/spark-dataframe-to-bucketed-table.drawio.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d48fea-56f1-4dad-a2f1-0ab615e127cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Создание таблиц для steam датафреймов\")\n",
    "\n",
    "games = bucketing(games_df, \"games\", \"app_id\")\n",
    "details = bucketing(details_df, \"details\", \"app_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a1b78-9bc1-49db-96ef-f2fa809eaf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Создание таблицы для taxi\")\n",
    "\n",
    "taxi = bucketing(taxi_df, \"taxi\", \"passenger_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5195202-d9f2-421b-81ab-894ce8c6f5db",
   "metadata": {},
   "source": [
    "## Базовые понятия"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd42eba-2a75-48a0-8280-74be902b81ae",
   "metadata": {},
   "source": [
    "### Номер бакета"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd975e60-07b9-4ad7-9cff-7d9faf29b653",
   "metadata": {},
   "source": [
    "Проверить в каком бакете окажется строка можно при помощи функции [`pmod`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.pmod.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11692513-eb3c-4903-a275-181e0995dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Получить распределение строк датафрейма `details_df` по бакетам\")\n",
    "\n",
    "details_buckets_pdf = (\n",
    "  details_df\n",
    "    .withColumn(\"hash\", hash(col(\"app_id\")))\n",
    "    .withColumn(\"bucket\", expr(\"pmod(hash, 10)\"))\n",
    "    .groupBy(\"bucket\").count()\n",
    "    .orderBy(\"bucket\")\n",
    "    .toPandas()\n",
    ")\n",
    "details_buckets_pdf.set_index(\"bucket\", inplace=True)\n",
    "_ = details_buckets_pdf.plot.pie(y=\"count\", autopct='%1.1f%%', legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4dcca6-337e-47a6-a0c1-2d18a92e15e3",
   "metadata": {},
   "source": [
    "В результате видно, что строки равномерно распределны по бакетам, т.к. по `app_id` нет перекоса в `details_df`. \n",
    "\n",
    "Но если в датафрейме есть перекос, как например в `taxi_df` по колонке `passenger_count`, то можно заметить, что большая часть значений попадает в один бакет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5a2fe4-fdea-4afd-ba37-06f27ac483ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Получить распределение по бакетам строк датафрейма `taxi_df`\")\n",
    "\n",
    "taxi_buckets_pdf = (\n",
    "  taxi_df\n",
    "    .withColumn(\"hash\", hash(col(\"passenger_count\")))\n",
    "    .withColumn(\"bucket\", expr(\"pmod(hash, 10)\"))\n",
    "    .groupBy(\"bucket\").count()\n",
    "    .orderBy(\"bucket\")\n",
    "    .toPandas()\n",
    ")\n",
    "taxi_buckets_pdf.set_index(\"bucket\", inplace=True)\n",
    "_ = taxi_buckets_pdf.plot.pie(y=\"count\", autopct='%1.1f%%', legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437ab3b9-2e94-4b3b-834b-2981bda80679",
   "metadata": {},
   "source": [
    "Функция [`pmod`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.pmod.html) принимает на вход два параметра: хеш и количество ожидаемых бакетов, а на выходе выдает, в каком бакете окажется запись.\n",
    "\n",
    "Функция [`pmod`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.pmod.html) не зависит от таблицы или датафрейма, а значит любые строки из разных таблиц окажутся в одном и том же бакете, если значение хеша у них совпадает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633f04e2-1a8a-47d5-a7db-99e3c8b38e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Демонстрация pmod\")\n",
    "\n",
    "spark.sql(\"select pmod(15, 5) id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb3dfe3-baac-47eb-b03c-e64a3e9c8ff0",
   "metadata": {},
   "source": [
    "На картике ниже схематически изображен выбор бакета:\n",
    "\n",
    "![](../imgs/spark-bucket-number.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c1fc90-394b-4236-a21d-772fc6b1a6b0",
   "metadata": {},
   "source": [
    "### Бакетирована ли таблица"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf509d7-746a-42ca-9c75-98ad01237b8f",
   "metadata": {},
   "source": [
    "Если нужно проверить бакетирована ли таблица, то можно посмотреть статистику по таблице, и в выводе будет указана информация о бакетировании:\n",
    "\n",
    "- `Num Buckets` - сколько бакетов,\n",
    "- `Bucket Columns` - по какой колонке выполнялось бакетирование."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890e29bc-7821-4f04-8205-63ef91769e41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DESCRIBE EXTENDED details\").show(200, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b0e977-d815-4a2a-8a86-7d8a85cf28ca",
   "metadata": {},
   "source": [
    "Для примера ниже создается новая таблица на базе `details_df` датафрейма, и в статистике нет информации о бакетировании, что говорит об отсутствии бакетов для таблицы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4411da3b-436a-450c-a301-557c832634ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Создать временную таблицу `details_temp_table`\")\n",
    "\n",
    "details_df.write.mode(\"overwrite\").saveAsTable(\"details_temp_table\")\n",
    "spark.sql(\"DESCRIBE EXTENDED details_temp_table\").show(200, False)\n",
    "_ = spark.sql(\"DROP TABLE details_temp_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a981d-866d-4c29-bfc0-3659c1e1a4c3",
   "metadata": {},
   "source": [
    "#### Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ef6759-e4f0-4b8c-8bba-495f32334387",
   "metadata": {},
   "source": [
    "Проверить на сколько бакетов и по какой колонке выполнялось бакетирование таблицы `taxi`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7043e2cc-a2d2-456c-9739-83299e13c151",
   "metadata": {},
   "source": [
    "## Использование бакетов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db62acc2-fd51-49c3-99b2-a88018d41db4",
   "metadata": {},
   "source": [
    "### Исключение партиций"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ff8365-0b88-496a-9823-476433d1ee24",
   "metadata": {},
   "source": [
    "Таблица `details` бакетирована по `app_id`, поэтому можно ожидать, что будет выполняться отсечение бакетов при сканировании.\n",
    "\n",
    "Для сравнения ниже приведен план запроса для `details_df`, который ничего не знает про бакеты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590746cb-3276-459b-b25b-9fd972ab239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Без бакетов: Выбрать строки по app_id = 1869690 из `details_df`\")\n",
    "\n",
    "df = details_df.where(\"app_id = 1869690\")\n",
    "df.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9541d1-0f15-4057-9f29-1bad5c57904e",
   "metadata": {},
   "source": [
    "Как видно из плана запроса, применилась оптимизация `PushDownFilters`: сканирование только нужных данных с диска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b997c7a2-be9e-4de8-a443-c84a7c91045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ae4786-d769-4d70-ba00-bfd9cef60f54",
   "metadata": {},
   "source": [
    "В случае работы с таблицей `details`, для которой создан отдельный датафрейм `details`, план запроса так же показывает, что отсечения по бакетам не произошло:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d8795e-9507-464d-8adc-710782c736de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"С бакетами: Выбрать строки по app_id = 1869690 из details\")\n",
    "\n",
    "df = details.where(col(\"app_id\") == F.lit(1869690).cast(LongType()))\n",
    "df.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159eb65a-11bb-4388-a16e-0e0b8e135101",
   "metadata": {},
   "source": [
    "Это связано с правилом [`DisableUnnecessaryBucketedScan`](https://github.com/apache/spark/blob/f1bc0f938162485a96de5788f53f9fa4fb37a3b1/sql/core/src/main/scala/org/apache/spark/sql/execution/bucketing/DisableUnnecessaryBucketedScan.scala#L77C8-L77C39), Catalyst решил, что есть боле быстрый способ получения данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf86ede0-f1c3-4836-b2d4-056aea902f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fc6666-802f-43d2-93ca-e6f550a8d89c",
   "metadata": {},
   "source": [
    "### Снижение shuffle операций в join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4595c70a-55ed-4ae5-b731-808a9bc74d14",
   "metadata": {},
   "source": [
    "В полной красе использование бакетов раскрывается при JOIN запросах."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb950af-a021-4bbc-a273-eaefd1c2c505",
   "metadata": {},
   "source": [
    "При объединении датафрейма с таблицей, по полю, по которому таблица разбита на бакеты, shuffle будет присутствовать только на стороне датафрейма:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb36c297-42ae-432c-b1d9-80ec85d53ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Соединение бакетированной таблицы и голого датафрейма: games_df + details\")\n",
    "\n",
    "df = games_df.join(details, \"app_id\")\n",
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eb822d-f28a-4a1a-ad6d-645c2bf92706",
   "metadata": {},
   "source": [
    "Такой тип объединения/join называется **one-side shuffle-free join**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb3eb33-5174-497a-93fb-0b494bde7a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4620572-3352-42ff-a80c-5074afd4c33d",
   "metadata": {},
   "source": [
    "### Join без shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b3ce37-f0b6-4bed-b60c-db0d24654439",
   "metadata": {},
   "source": [
    "Но полностью получить преимущества бакетов в JOIN запросах можно, если соединение двух бакетированных таблиц выполняется по колонке бакетирования. В плане исполнения узел `Exchange` будет отсутствовать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4791fd-fec1-48c8-9609-3923154aa65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Join без shuffle: details + games по app_id\")\n",
    "\n",
    "df = details.join(games, \"app_id\")\n",
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3de80c-d201-4cef-ba1d-e699f19d16fc",
   "metadata": {},
   "source": [
    "В плане запроса нет традиционного `Exchange` узла, который обычно присутствует перед операцией `Sort` при `Sort Merge Join` стратегии."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05464527-540c-43c5-a27e-3433b4756432",
   "metadata": {},
   "source": [
    "В дополнении к привычному плану можно увидеть инструкцию `SelectedBucketsCount: 100 out of 100`, которая показывает какие бакеты были просканированы. В данном случае были просканированы все 100 бакетов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6b03dd-9624-4339-aa3b-c12dfdcb9921",
   "metadata": {},
   "source": [
    "![](../imgs/spark-buckets-join.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1353f60d-86d5-4473-9120-924f6f486054",
   "metadata": {},
   "source": [
    "Производительность запроса можно поднять еще выше, если добавить условие по бакетированной колонке, так можно сразу исключить ненужные бакеты из анализа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afa17a1-548d-42c5-834b-760db8d60d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Исключение бакетов в Join без shuffle: details + games по app_id == 1869690\")\n",
    "\n",
    "df = (\n",
    "  games\n",
    "    .join(details, \"app_id\")\n",
    "    .where(col(\"app_id\") == 1869690)\n",
    ")\n",
    "\n",
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36292438-2ee2-4154-8b64-81dcfcdf494f",
   "metadata": {},
   "source": [
    "В плане запроса можно увидеть инструкцию `SelectedBucketsCount: 1 out of 100`, т.е. был просканирован только один бакет из ста. Строки с одинаковым значением ключа бакетирования попадут в один и тот же бакет. Номер бакета можно вычислить заранее, а значит нет необходимости сканировать остальные бакеты.\n",
    "\n",
    "![](../imgs/spark-buckets-join-filter.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6b295f-5aa5-4512-93d9-e0d47e733036",
   "metadata": {},
   "source": [
    "Для сравнения можно выполнить аналогичный запрос, но уже на голых датафреймах, которые ничего не знают о бакетах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0660e936-75d5-4d97-a2a7-92facffef5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Join датафреймов details_df + games_df по app_id == 1869690 (только PushedFilters)\")\n",
    "\n",
    "df = (\n",
    "  details_df\n",
    "    .join(games_df, \"app_id\")\n",
    "    .where(col(\"app_id\") == 1869690)\n",
    ")\n",
    "\n",
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2263474-9b5f-4d90-8936-4b7933d8903d",
   "metadata": {},
   "source": [
    "Бакеты так же будут использоваться и при **HASH JOIN**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9735dfca-4d51-4688-a279-14d6f04d699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"HASH JOIN: нет shuffle и сканируется только один бакет\")\n",
    "\n",
    "df = (\n",
    "  details.hint(\"SHUFFLE_HASH\")\n",
    "    .join(games, \"app_id\")\n",
    "    .where(col(\"app_id\") == 1869690)\n",
    ")\n",
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af2d618-74e9-49a6-9127-735b2e03088f",
   "metadata": {},
   "source": [
    "В плане видно:\n",
    "\n",
    "- отсутствие узла `Exchange`,\n",
    "- сканирование только одного бакета, в котором могут быть интересующие строки."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08dbfe6-6a3d-4a7e-b806-383ce874c75c",
   "metadata": {},
   "source": [
    "Бакеты игнорируются при использовании `BroadcastHashJoin` в силу особенностей работы алгоритма:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c23bd3-dff5-4459-92db-97e5550e1ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"BROADCAST JOIN: бакеты не используются\")\n",
    "\n",
    "df = (\n",
    "  details.hint(\"BROADCAST\")\n",
    "    .join(games, \"app_id\")\n",
    "    .where(col(\"app_id\") == 1869690)\n",
    ")\n",
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f96bc6-1467-4544-bef6-e76d91248fbc",
   "metadata": {},
   "source": [
    "### Сортировка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6468fb27-6307-4a3f-b9bf-1daa6c7771b1",
   "metadata": {},
   "source": [
    "При использовании `Sort Merge Join` по прежнему можно видеть узел `Sort` в плане запроса. От этой операции так же можно избавиться при помощи предварительной сортировки строк в бакетах. Но и тут есть свои особенности."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1ed437-c3e1-4a88-9a5a-1cc529188014",
   "metadata": {},
   "source": [
    "Функция `bucketing_single_partition` создает таблицу на базе датафрейма с одной партицией:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0ca885-d70d-45ec-80ff-5a15f683bc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucketing_single_partition(df: DataFrame, table_name: str, column: str, buckets: int = 100) -> DataFrame:\n",
    "    (\n",
    "    df.repartition(1)\n",
    "      .write\n",
    "      .mode(\"overwrite\")\n",
    "      .bucketBy(buckets, column)\n",
    "      .sortBy(column)\n",
    "      .saveAsTable(table_name)\n",
    "    )\n",
    "    return spark.table(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae78b331-a05c-44c5-8dd4-cfa43bc879d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Создание таблиц steam из датафреймов с одной партицией\")\n",
    "\n",
    "details_single_partition = bucketing_single_partition(details_df, \"details_single_partition\", \"app_id\")\n",
    "games_single_partition = bucketing_single_partition(games_df, \"games_single_partition\", \"app_id\")\n",
    "tags_single_partition = bucketing_single_partition(tags_df, \"tags_single_partition\", \"tag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc608c8-1c0b-448d-8c7f-368525004d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DESC EXTENDED details_single_partition\").show(40, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64827547-2ea5-43d8-90e0-5ed0256cc374",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DESC EXTENDED games_single_partition\").show(40, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dcb812-865e-41af-9b08-ee37f075d308",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Соединение details + games: Сортировка присуствует\")\n",
    "\n",
    "details.join(games, \"app_id\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e2b7c0-50d4-4b63-9d34-dfcb29f73865",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Соединение details_single_partition + games_single_partition: Сортировка присуствует\")\n",
    "\n",
    "details_single_partition.join(games_single_partition, \"app_id\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7031c51d-3fb7-4ede-8279-c852168abc6d",
   "metadata": {},
   "source": [
    "Кажется, что независимо от того сколько партиций в датафрейме, от сортировки нет эффекта: узел `Sort` как был в плане, так и остался. Но не все так просто.\n",
    "\n",
    "До версии 3.0 Apache Spark действительно использовал подготовленную сортировку в бакетах, но в версии 3.0 эту возможность спрятали за настройкой `spark.sql.legacy.bucketedTableScan.outputOrdering`, и теперь она отключена по умолчанию ([подробности](https://github.com/apache/spark/pull/25328)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139918a1-c6cd-4c66-a444-97260ed7eb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.legacy.bucketedTableScan.outputOrdering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be639894-2737-4fb7-9c83-ce60cacafea0",
   "metadata": {},
   "source": [
    "Если включить эту настройку, то из плана запроса пропдают узлы `Sort`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aae705-5844-4aac-8ac3-f36800a117e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Соединение details + games: Сортировка отсуствует\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.bucketedTableScan.outputOrdering\", True)\n",
    "\n",
    "details_single_partition.join(games_single_partition, \"app_id\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b31e1bd-1f29-41fa-a632-9c0fffff76e0",
   "metadata": {},
   "source": [
    "Но это действует только на датафреймы с одной партицией, если в датафрейме больше одной партиции, то сортировка остается:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5165212f-0fb0-4e30-bc34-d2b898a116f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Соединение details + games: Сортировка присуствует\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.bucketedTableScan.outputOrdering\", True)\n",
    "\n",
    "details.join(games, \"app_id\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4301b7e2-c1ab-4163-a5b8-9f8ce84c872f",
   "metadata": {},
   "source": [
    "#### Почему так?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02cebce-c0ba-4889-b3f5-c9d864ddd188",
   "metadata": {},
   "source": [
    "Это связано с тем, что одна партиция будет разбита на `N` бакетов, где `N` - общее число бакетов таблицы. Если указать 100 бакетов, то для датафрейма из четырех партиций создастся 400 файлов: по четыре файла на бакет, по одному файлу на партицию.\n",
    "\n",
    "Сортировка сохраняется только в пределах одного файла, глобальной сортировки на весь бакет нет.\n",
    "\n",
    "![](../imgs/spark-global-sorting.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf84696-4aed-4ee2-8ba3-5bef25b52d66",
   "metadata": {},
   "source": [
    "С другой стороны, если один бакет состоит из одного файла, а это возможно только, если весь оригинальный датафрейм находился в одной партиции, то можно переиспользовать эту сортировку:\n",
    "\n",
    "![](../imgs/spark-single-file-bucket-sorting.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4806f47-018c-47c2-8547-6c9233cce712",
   "metadata": {},
   "source": [
    "#### Зачем тогда вообще сортировка?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07dc2de-9102-47a7-9b2f-a0ed09e36fd1",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>А, правда, зачем?</summary>\n",
    "\n",
    "    Сортировка позволяет исползовать бинарный поиск при получении строк из бакета\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef87ccb2-f856-4f54-8495-ecde3a7af819",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Аггрегаты без shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d138a2b-981b-4071-beba-e97638dfffce",
   "metadata": {},
   "source": [
    "Преимущества отсечения данных при сканировании бакетов можно также получить при работе с агрегатами:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe6663e-75d8-41b3-8b69-d9470e589f5c",
   "metadata": {},
   "source": [
    "![](../imgs/spark-bucket-aggregation.drawio.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5605fc0e-4c5c-4684-8eb3-34ced28f868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Агрегаты с бакетами: details.count\")\n",
    "\n",
    "details.where(\"app_id == 123\").groupBy(\"app_id\").count().explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1a50e2-0688-4a2e-b88f-1494aae237a3",
   "metadata": {},
   "source": [
    "В плане выше можно увидеть, что Catalyst использует бакеты `SelectedBucketsCount: 1 out of 100`, чего не наблюдается при использовании голых датафреймов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e4932a-bbd5-442d-a10c-3936db13c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Агрегаты без бакетов: details_df.count\")\n",
    "\n",
    "details_df.where(\"app_id == 123\").groupBy(\"app_id\").count().explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f20d6a-7edb-44f6-b1c1-88f823e60980",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Оконные функции без shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff270c2c-ec83-4d77-9144-77b26f3a0b23",
   "metadata": {},
   "source": [
    "Оконные функции также получают преимущества от бакетов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e33d822-7594-4e4f-a626-8d763418a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Оконные функции: details.count\")\n",
    "\n",
    "df = (\n",
    "  details\n",
    "    .withColumn(\n",
    "      'cnt',\n",
    "      F.count('*')\n",
    "        .over(Window().partitionBy('app_id'))\n",
    "    )\n",
    "    .where(col(\"app_id\") == 123)\n",
    "  )\n",
    "df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90428ca2-9fe2-4323-846d-6c6e109a3b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Оконные функции: details_single_partition.count\")\n",
    "\n",
    "df = (\n",
    "  details_single_partition\n",
    "    .withColumn(\n",
    "      'cnt',\n",
    "      F.count('*')\n",
    "        .over(Window().partitionBy('app_id'))\n",
    "    )\n",
    "    .where(col(\"app_id\") == 123)\n",
    "  )\n",
    "df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c2d50a-e368-4a77-a1fc-07047f61b3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Оконные функции: details_df.count\")\n",
    "\n",
    "df = (\n",
    "  details_df\n",
    "    .withColumn(\n",
    "      'cnt',\n",
    "      F.count('*')\n",
    "        .over(Window().partitionBy('app_id'))\n",
    "    )\n",
    "    .where(col(\"app_id\") == 123)\n",
    "  )\n",
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fade0f-157d-4d2c-b736-14b157986e1b",
   "metadata": {},
   "source": [
    "## Особенности"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddfab79-9579-45d0-af0e-5bc113970c3d",
   "metadata": {},
   "source": [
    "Бакеты дают огромный прирост производительности, но иногда бакеты могут не активироваться, поэтому важно понимать особенности их работы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3983cbd0-4023-45a8-b40c-4dd6747beeeb",
   "metadata": {},
   "source": [
    "### Неравное количество бакетов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d26bf9-bf02-48d9-b526-68a1bf64ea2a",
   "metadata": {},
   "source": [
    "Не всегда есть возможность разбить таблицы на одинаковое число бакетов. При неравном числе бакетов таблиц, участвующих в соединении, Apache Spark может все еще воспользоваться бакетами, но есть ограничения.\n",
    "\n",
    "Apache Spark при помощи алгоритма `coalesce` может соединить несколько бакетов в один, но выполнять он это будет попарно. Следовательно, если таблицы разбиты на неравное число бакетов, то меньшее число бакетов должно быть делителем большего числа бакетов. В этом случае Apache Spark может снизить бОльшее число бакетов до меньшего числа бакетов, а дальше воспользоваться обычным алгоритмом работы с бакетами.\n",
    "\n",
    "![](../imgs/spark-coalesce-buckets.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2d9f7d-888b-4895-bf73-d9689e30e413",
   "metadata": {},
   "source": [
    "Таблица `details` будет иметь 25 бакетов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec28eeb-92bd-4907-9b09-25e601b073f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Число бакетов details == 25\")\n",
    "\n",
    "details = bucketing(details_df, \"details\", \"app_id\", 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1d63a4-7466-4bcb-8823-bee54e2a5868",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DESC EXTENDED details\").where(\"col_name == 'Num Buckets'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d32b82-1afb-44b0-b47d-d43051218395",
   "metadata": {},
   "source": [
    "Таблица `games` состоит из 100 бакетов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e965cbb-8ddc-486d-b3db-580bf5c6486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DESC EXTENDED games\").where(\"col_name == 'Num Buckets'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c380ce83-bf1c-40c9-9825-3d57c52b3f4d",
   "metadata": {},
   "source": [
    "25 является делителем 100, поэтому Apache Spark должен свободно воспользоваться бакетами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb0c91e-e43e-4c66-9b5f-09dd982a0d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Слияние бакетов недоступно: games + details\")\n",
    "\n",
    "(\n",
    "games\n",
    "  .join(details, \"app_id\")\n",
    "  .explain()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b047861c-aa15-40dd-8ecc-a4464c9576c4",
   "metadata": {},
   "source": [
    "Но на практике этого не выходит почему-то..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4399f4c6-47de-4819-abe7-aadbd42e88e7",
   "metadata": {},
   "source": [
    "Также не получается воспользоваться бакетами, если явно поменять число партиций на 25:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9937f0-98a3-4bf4-b0b1-13909128845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Слияние бакетов недоступно: изменение числа партиций games\")\n",
    "\n",
    "(\n",
    "games.repartition(25, \"app_id\")\n",
    "  .join(details, \"app_id\")\n",
    "  .explain()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3cf665-431a-4697-bed3-01007aebd75f",
   "metadata": {},
   "source": [
    "Бакеты и партиции разные вещи, поэтому явное изменение числа партиций просто создало новый датафрейм, у которого 25 партиций, он уже никакого отношения не имеет к таблице, которая разбита на 100 бакетов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8325580-7290-4db8-a2d3-ac4dbbe77711",
   "metadata": {},
   "source": [
    "Механизм автоматического слияния бакетов для достижения однакого числа бакетов в соединяемых таблицах контролируется настройкой `spark.sql.bucketing.coalesceBucketsInJoin.enabled`, которая выключена по умолчанию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b58e3e4-54ae-4ffb-8af0-b01b3b4cf43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.bucketing.coalesceBucketsInJoin.enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e834f1-6b21-4fd5-80e6-862aadce5c61",
   "metadata": {},
   "source": [
    "После ее включения Spark должен начать склеивать партиции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513f6d06-a0e4-407b-a166-f14d13d37a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Слияние бакетов недоступно: games + details (bug)\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.bucketing.coalesceBucketsInJoin.enabled\", True)\n",
    "\n",
    "(\n",
    "games\n",
    "  .join(details, \"app_id\")\n",
    "  .explain()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19780410-2873-43e4-be59-d2acd745c6cd",
   "metadata": {},
   "source": [
    "Но этого не происходит из-за [ошибки в логике](https://issues.apache.org/jira/browse/SPARK-43021) оптимизатора: склеивание бакетов не происходит при включенном AQE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc597873-0d1d-4e00-9834-3ffc06d8be08",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Слияние бакетов работает: games + details (без AQE)\")\n",
    "\n",
    "# автоматическое склеивание партиций не работает при включенном AQE. Обновления в https://github.com/apache/spark/pull/40688\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "(\n",
    "games\n",
    "  .join(details, \"app_id\")\n",
    "  .explain()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58509e13-9492-4280-a390-2cae5669418b",
   "metadata": {},
   "source": [
    "В плане выше можно заметить:\n",
    "\n",
    "- операции `Exchange` отсутствуют,\n",
    "- бакеты таблицы `games` были склеены по два так, что получилось в итоге 25 бакетов `SelectedBucketsCount: 100 out of 100 (Coalesced to 25)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae244ee-ad64-4d10-b96b-0c1d21d793d9",
   "metadata": {},
   "source": [
    "Полное отключение AQE - плохая идея, поэтому после запроса необхдимо включить его назад:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1581b7a3-58be-454d-92da-655c8a92ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66254f75-99f1-4882-9b38-be2ab9a86f84",
   "metadata": {},
   "source": [
    "Не любое число бакетов может быть склеено. Склеивание будет выполняться только, если отншение бОльшего числа бакетов к меньшему не превышает порогового значения указанного в `spark.sql.bucketing.coalesceBucketsInJoin.maxBucketRatio`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd7531b-91cb-4cf9-b6a7-d9706c0b65f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.bucketing.coalesceBucketsInJoin.maxBucketRatio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf376e00-557e-45f6-a698-63a37ce6fcae",
   "metadata": {},
   "source": [
    "По умолчанию используется значение 4. Сейчас отношение числа бакетов `games` к числу бакетов `details` 2:1, поэтому склеивание произошло успешно:\n",
    "\n",
    "![](../imgs/spark-many-coalesce-buckets.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47111a6a-9c3a-4cfc-b621-0176742b4b3e",
   "metadata": {},
   "source": [
    "Если указать 2 в качества порогового значения, то склеивание бакетов выполняться не будет, $log_2(2) = 1$, т.е. можно склеить бакеты один раз, а нужно два раза: $100 \\rightarrow 50 \\rightarrow 25$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc79238c-e48e-4660-bc95-35885ef091aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Слияние бакетов недоступно: maxBucketRatio превышен\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.bucketing.coalesceBucketsInJoin.maxBucketRatio\", 2)\n",
    "\n",
    "# автоматическое склеивание партиций не работает при включенном AQE. Обновления в https://github.com/apache/spark/pull/40688\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "(\n",
    "games\n",
    "  .join(details, \"app_id\")\n",
    "  .explain()\n",
    ")\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", True)\n",
    "spark.conf.set(\"spark.sql.bucketing.coalesceBucketsInJoin.maxBucketRatio\", 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be57c94-2f1b-4fac-8f9e-e45face73ffe",
   "metadata": {},
   "source": [
    "В плане снова можно увидеть узел `Exchange`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae26041-cda1-4b0d-b2e5-86426080cdf1",
   "metadata": {},
   "source": [
    "Бакеты можно также использовать и в Spark SQL запросах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef69158-c14d-464b-839b-5fff06e45154",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Слияние бакетов работает: Spark SQL\")\n",
    "\n",
    "# автоматическое склеивание партиций не работает при включенном AQE. Обновления в https://github.com/apache/spark/pull/40688\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "\n",
    "spark.sql(\"select /*+ SHUFFLE_HASH(u) */ * from games join details using (app_id)\").explain()\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dccc4c2-eb14-4e9f-bce5-e62eb5ece67a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39301533-6324-410a-9daa-8c5291395581",
   "metadata": {},
   "source": [
    "Spark хранит метаинформацию о том, что конкретная таблица имеет конкретное количество бакетов, поэтому необходимо создавать датафрейм из таблицы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c17cb60-3e03-4818-a28f-4c9f7b1d6500",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Загрузка данных: загрузка таблицы\")\n",
    "\n",
    "df = (\n",
    "  spark\n",
    "    .table(\"details\")\n",
    "    .groupBy(\"app_id\")\n",
    "    .count()\n",
    ")\n",
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf2f08c-292d-47d2-be73-bcbe0d0731ce",
   "metadata": {},
   "source": [
    "При этом если загрузить данные просто как parquet, то бакетинг использоваться не будет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e51d74-7321-4b26-a30e-d75c448a4c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Загрузка данных: чтение parquet\")\n",
    "\n",
    "df = (\n",
    "  spark\n",
    "    .read.parquet('/tmp/spark-warehouse/details')\n",
    "    .groupby('app_id')\n",
    "    .count()\n",
    ")\n",
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc0ce31-8446-439d-8327-8f3a8a30b578",
   "metadata": {},
   "source": [
    "### Разные типы данных ключа"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d02d013-747a-4dd5-9d31-a33980586ba7",
   "metadata": {},
   "source": [
    "Одной из самых неприятных проблем, с которой можно столкнуться - это разница типов ключей бакетирования в соединяемых таблицах: Apache Spark не будет автоматически приводить `int` к `long` или `double`, а просто откажется использовать бакеты.\n",
    "\n",
    "В качестве примера ниже создается датафрейм, в котором `app_id` имеет тип `int`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8903ae8c-3570-4868-b580-11466548ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Поменять тип `app_id`: long -> int\")\n",
    "\n",
    "games_df_int_key = (\n",
    "  games_df\n",
    "    .withColumn(\n",
    "      \"app_id\",\n",
    "      col(\"app_id\").cast(\"int\")\n",
    "    )\n",
    ")\n",
    "games_df_int_key.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3506417-2947-4289-b72a-826940018738",
   "metadata": {},
   "source": [
    "На базе датафрейма `games_df_int_key` создается новая бакетированная таблица:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde7b21-ac26-4400-ab1f-47c381f6c23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Создание таблицы `games_int_key`, бакетированной по `app_id`\")\n",
    "\n",
    "games_int_key = bucketing(games_df_int_key, \"games_int_key\", \"app_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9fa79c-1ef7-4d40-8ed8-ee5476448640",
   "metadata": {},
   "source": [
    "Запрос ниже показывает, что бакеты для новой таблицы активировались:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89827413-89db-47a5-9370-3a0a0289703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Проверка бакетов таблицы `games_int_key`\")\n",
    "\n",
    "games_int_key.groupBy(\"app_id\").count().explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c276c30e-d224-428b-938f-4f0f847f3b45",
   "metadata": {},
   "source": [
    "Но если соединить таблицу `games_int_key` c таблицей `details`, то в результате можно увидеть узлы `Exchange`, что говорит о том, что Apache Spark не использует бакеты полноценно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202ee403-c8d6-4929-92ea-84bfdb85c388",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Соединение games_int_key + details: бакеты не используются\")\n",
    "\n",
    "games_int_key.join(details, \"app_id\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c826f512-2162-448d-9666-e808f73502da",
   "metadata": {},
   "source": [
    "Решением будет изменение типа колонки `app_id` в `games_int_key` на `long` или типа колонки `app_id` в `details` на `int`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbb79a1-b588-40bb-9087-4f60f1f45fb2",
   "metadata": {},
   "source": [
    "### Пользовательские функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df247a50-2bcd-4c96-9801-e793a7739b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Вернуть схему бакетирования для `details`\")\n",
    "\n",
    "details = bucketing(details_df, \"details\", \"app_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5415b219-1ae2-42c4-9756-366058d120be",
   "metadata": {},
   "source": [
    "Пользовательские функцию могут препятствовать использованию бакетов.\n",
    "\n",
    "Например, ниже определена булевая фукнция, которая определяет много или мало отзывов у игры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9dcfc8-bb83-48c8-a9f1-bd9e452abbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_reviews = lambda reviews: reviews > 5\n",
    "many_reviews_udf = F.udf(many_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c752e4-9486-4854-87d8-bcd7d92e63fa",
   "metadata": {},
   "source": [
    "Если применить эту функцию к таблице перед соединением, то информация о бакетах не используется, и оптимизатор добавляет узел `Exchange` в план запроса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969fab75-0407-457e-b3b6-a1fcae03792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"UDF мешает бакетам\")\n",
    "\n",
    "(\n",
    "  games.withColumn(\n",
    "      \"many_reviews\",\n",
    "      many_reviews_udf(col(\"user_reviews\"))\n",
    "    )\n",
    "    .join(details, \"app_id\")\n",
    "    .explain()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5130fbe5-dacf-41b3-beeb-fd5c09e940c7",
   "metadata": {},
   "source": [
    "Решением будет применение функции после соединения таблиц:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2e7a71-06ec-4772-a402-558c33783b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Бакеты работают с UDF\")\n",
    "\n",
    "(\n",
    "  games\n",
    "    .join(details, \"app_id\")\n",
    "    .withColumn(\n",
    "      \"many_reviews\",\n",
    "      many_reviews_udf(col(\"user_reviews\"))\n",
    "    )\n",
    "    .explain()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce95571-e5a8-4741-b240-122d9aac3eb5",
   "metadata": {},
   "source": [
    "### Динамическое партиционирование"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c1cd0e-ecb3-48bd-9da8-5e7616290bb4",
   "metadata": {},
   "source": [
    "Бакеты используются с высококардинальными признаками/колонками. Если колонка может содержать небольшое число различных значений, то лучше подойдет динамическое партционирование. Партиционирование можно комбинировать с бакетами:\n",
    "\n",
    "![](../imgs/spark-partitioning.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199ae4e7-2738-4753-bca2-00d4c6fad2d7",
   "metadata": {},
   "source": [
    "Для демонстрации динамического партиционирования создается новый датафрейм `games_part_by_year_df` на базе датафрейма `games_df`, который будет партиционирован по году релиза игры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b53e87-4a73-4cd8-8f0d-72dd37b7627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Динамическое партиционирование: создать `games_part_by_year_df` с колонкой `year`\")\n",
    "\n",
    "games_part_by_year_df = games_df \\\n",
    "  .withColumn(\n",
    "    \"year\",\n",
    "    F.date_format(col(\"date_release\"), \"y\").cast(\"int\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd61713a-868e-4d93-be90-fb5ae39fcc30",
   "metadata": {},
   "source": [
    "Функция `bucketing_partition` разбивает датафрейм на партиции, а каждая партиция еще дополнительно разбивается на бакеты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241612ec-0d67-4ab5-b9a1-2e4166cc8dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucketing_partition(df: DataFrame, table_name: str, column: str, partition: str, buckets: int = 100) -> DataFrame:\n",
    "    (\n",
    "    df.write\n",
    "      .mode(\"overwrite\")\n",
    "      .partitionBy(partition)\n",
    "      .bucketBy(buckets, column)\n",
    "      .sortBy(column)\n",
    "      .saveAsTable(table_name)\n",
    "    )\n",
    "    return spark.table(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc49533-b2e0-4a82-b878-cc04576342fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Создание таблицы `games_part_by_year`\")\n",
    "\n",
    "games_part_by_year = bucketing_partition(\n",
    "    games_part_by_year_df,\n",
    "    \"games_part_by_year\",\n",
    "    \"app_id\",\n",
    "    \"year\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc25ba0c-8cb0-4092-91c7-bc85b4673269",
   "metadata": {},
   "source": [
    "При запросе данных из партиционированной таблицы по ключу партиционирования, оптимизатор применит правило `PartitionFilters` при сканировании таблицы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622b94b3-73bb-47a6-851d-0b6b8bceee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Динамическое партиционирование: PartitionFilers по колонке `year`\")\n",
    "\n",
    "df = games_part_by_year.where('year == 2012')\n",
    "df.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003f7a3d-f0e5-46a0-8b76-5d1f91ddc55b",
   "metadata": {},
   "source": [
    "### Обновление данных в таблице"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b13b913-8404-4eef-9739-15af2535d6ae",
   "metadata": {},
   "source": [
    "В таблицу `games_part_by_year` будут добавлены новые записи за следующий год:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d174ed-bb99-40aa-b891-0466f82f3662",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Получить максимальный год в `games_part_by_year`\")\n",
    "\n",
    "[[year]] = games_part_by_year.select(F.max(\"year\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8604f96b-b1d8-49ce-983c-ddf28b27b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "year += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c35372-bc1b-4258-b0f0-df4e9678c909",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Подготовить новые данные для `games_part_by_year`\")\n",
    "\n",
    "new_data = (\n",
    "  games_part_by_year\n",
    "    .where(col('year') == year - 10)\n",
    "    .withColumn('date_release', F.add_months(col('date_release'), 10 * 12))\n",
    "    .withColumn('year', F.lit(year))\n",
    "    .limit(5)\n",
    "  )\n",
    "\n",
    "new_data.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadfb4d8-f8e9-42ae-8d75-289995284329",
   "metadata": {},
   "source": [
    "Вставить подготовленные данные в `games_part_by_year`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b05410-7d14-46b5-ade2-0ec86914430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Вставить подготовленные данные в `games_part_by_year`\")\n",
    "\n",
    "new_data.write.insertInto(\"games_part_by_year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b34897e-d7f1-4d4f-84cc-357160b72882",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Получить вставленные данные\")\n",
    "\n",
    "df = games_part_by_year.where(col(\"year\") == year)\n",
    "df.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c79bc0-f807-4c59-b8a4-0b2b7b2b9ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdc6ab6-1e61-4a2b-ab50-168de1a01a76",
   "metadata": {},
   "source": [
    "### Немного математики"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e470969a-51f8-4ec4-b4f6-99b63571a4a5",
   "metadata": {},
   "source": [
    "В итоге в каждой партиции количество файлов определяется формулой:\n",
    "$$\n",
    "Количество\\spaceфайлов\\spaceв\\spaceпартиции = Количество\\spaceбакетов \\times Количество\\spaceпартиций\\spaceоригинального\\spaceдатафрейма\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad6b5b8-29fe-490b-99a6-90fc650d9f0b",
   "metadata": {},
   "source": [
    "**Дано:**\n",
    "\n",
    "- количество партиций в датафрейме `games_part_by_year_df`: 4;\n",
    "- количество бакетов: 100.\n",
    "\n",
    "Тогда число файлов в партиции будет равно 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd9f4a9-67c2-49b9-a2af-bd6330febc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find /tmp/spark-warehouse/games_part_by_year/*2015* -type f -name \"*.parquet\" | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9fa56f-a53d-40d2-b94d-ce0f7bf14d82",
   "metadata": {},
   "source": [
    "А всего файлов для итоговой таблицы будет в $|X|$ раз больше, где $|X|$ мощность множества колонки партиционирования (количество уникальных значений):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f21da1d-0bf3-4f11-a5d1-7dd9e4e7a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find /tmp/spark-warehouse/games_part_by_year/ -type f -name \"*.parquet\" | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f92ffd6-d698-410a-b690-4f5947ddf252",
   "metadata": {},
   "source": [
    "Эту особенность нужно учитывать, т.к. сканирование большого числа небольших файлов может сильно снизить производительность запроса."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7446c063-6088-4953-8e8d-f4c04658d823",
   "metadata": {},
   "source": [
    "### Конфигурация параллельного сканирования файлов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2d351c-dca1-4d02-b827-3424f227c3e2",
   "metadata": {},
   "source": [
    "Сканирование файлов с диска будет выполняться параллельно, если число файлов превышает пороговое значение (по умолчанию 32). Пороговое значение можно настраивать:\n",
    "\n",
    "- [`spark.sql.sources.parallelPartitionDiscovery.threshold`](https://github.com/apache/spark/blob/834d71b990468006ae4b1df17fae31f639d0d7ff/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala#L1621) - сколько файлов в директории активирует параллельный режим обхода директории и запускает задачу обхода директории на кластере,\n",
    "- [`spark.sql.sources.parallelPartitionDiscovery.parallelism`](https://github.com/apache/spark/blob/834d71b990468006ae4b1df17fae31f639d0d7ff/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala#L1633) - максимальныое число заданий (task) при чтении данных. Желательно настраивать, чтобы заданий (tasks) не было слишком много."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d99999-5832-432e-af96-fcb7e119e172",
   "metadata": {},
   "source": [
    "## Конфигурация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb300c84-64f6-49b2-82a0-20be48cb63c1",
   "metadata": {},
   "source": [
    "| Настройка | Описание | Значение по умолчанию |\n",
    "| --------- | -------- | --------------------- |\n",
    "| [`spark.sql.sources.bucketing.enabled`](https://github.com/apache/spark/blob/834d71b990468006ae4b1df17fae31f639d0d7ff/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala#L1483) | Включить поддержку бакетов | `True` |\n",
    "| [`spark.sql.sources.bucketing.maxBuckets`](https://github.com/apache/spark/blob/834d71b990468006ae4b1df17fae31f639d0d7ff/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala#L1545) | Максимальное количество бакетов на таблицу | `100 000` (100 тысяч) |\n",
    "| [`spark.sql.sources.bucketing.autoBucketedScan.enabled`](https://github.com/apache/spark/blob/834d71b990468006ae4b1df17fae31f639d0d7ff/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala#L1553) | Исключить использование бакетов из плана запроса, если без них эффективнее | `True` |\n",
    "| [`spark.sql.bucketing.coalesceBucketsInJoin.enabled`](https://github.com/apache/spark/blob/834d71b990468006ae4b1df17fae31f639d0d7ff/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala#L4120) | Если две таблицы при join имеют разное количество бакетов, слить (coalesce) несколько бакетов большей таблицы в один, чтобы уровнять число бакетов. Сработает только, если меньшее число бакетов является делителем большего (100 и 50, например) | `False` |\n",
    "| [`spark.sql.bucketing.coalesceBucketsInJoin.maxBucketRatio`](https://github.com/apache/spark/blob/834d71b990468006ae4b1df17fae31f639d0d7ff/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala#L4133) | Во сколько раз сколько количество бакетов может отличаться, чтобы выполнить слияние согласно настройке `spark.sql.bucketing.coalesceBucketsInJoin.enabled` (см. выше). Число шагов слияния будет не больше $log_2(spark.sql.bucketing.coalesceBucketsInJoin.enabled)$ | `4` (не больше чем в 4 раза) |\n",
    "| [`spark.sql.legacy.bucketedTableScan.outputOrdering`](https://github.com/apache/spark/blob/834d71b990468006ae4b1df17fae31f639d0d7ff/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala#L3623) | Исключить сортировку из плана запроса, если бакет состоит из одного файла, и все данные в нем отсортированы | `False` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3941895-8d5d-4dda-8f86-51dfefe13b50",
   "metadata": {},
   "source": [
    "## Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365112e5-da6e-4c3d-ae7d-b1172937f814",
   "metadata": {},
   "source": [
    "При классической OLAP нагрузке, где данные записываются один раз, а считываются много раз, использование бакетов может значительно увеличить производительность запросов. Но, как и многие техники оптимизации, бакетирование не является универсальным способом, подходящим для любых нагрузок. К основным недостаткам можно отнести:\n",
    "\n",
    "- появление большого числа файлов,\n",
    "- ограниченность типа запросов.\n",
    "\n",
    "Много файлов само по себе может и не быть проблемой, но если файлы имеют небольшой размер, то для HDFS это может стать большой проблемой. NameNode может хранить ограниченное число файлов, хотя и очень большое, что диктует максимальный объем хранилища.\n",
    "\n",
    "При создании бакетированной таблицы, очень важно выполнять запросы по ключу бакетирования, чтобы отсекать ненужные файлы заранее. В противном случае, для запросов без ключа бакетирования Apache Spark будет вынужен запускать сканирование всех файлов таблицы. Одно из решений - создание отдельной таблицы со своим ключом бакетирования для каждого типа запроса. Но нужно быть внимательным, т.к. в таком случае данные будут дублироваться в нескольких таблицах. Синхронизация данных относится техникам организации озера данных (Data Lake), которое будет разбираться позднее.\n",
    "\n",
    "<details>\n",
    "    <summary><b>Spoiler Alert</b></summary>\n",
    "\n",
    "    Для синхронизации данных в таблицах можно использовать Structured Streaming, и это единственное приемлемое применение Structured Streaming.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cd973c-d520-47f6-9115-78e643054f1e",
   "metadata": {},
   "source": [
    "## Задания"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c523a88-67f4-463b-8901-238ca33550ca",
   "metadata": {},
   "source": [
    "1. На базе датафрейма `tags_df` создать таблицу `tags` с ключом бакетирования `tag`;\n",
    "1. На базе датафрейма `details_df` создать таблицу `details_by_tag` с ключом бакетирования `tag`;\n",
    "1. Выполнить join между `details_by_tag` и `tags` по полю `tag`, убедиться что бакеты используются;\n",
    "1. Посчитать количество строк по каждому тегу в таблице с деталями, проанализировать план;\n",
    "1. Составить наиболее оптимальный join между таблицами с тегами, деталями и играми. Подсказка: возможно понадобяться дополнительные колонки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90672e13-88bb-4be4-99db-c26e05265879",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
