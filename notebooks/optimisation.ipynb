{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b37be800-73ac-4f2f-8974-20fbe29fe302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages io.dropwizard.metrics:metrics-servlets:4.2.0 pyspark-shell'\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config(\"spark.sql.autoBroadcastJoinThreshold\", 0). \\\n",
    "    appName(\"Spark Optimizations\"). \\\n",
    "    master(\"local[4]\"). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5780f42-8fd4-4976-97bd-24a2f6b0639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = spark.read \\\n",
    "  .format(\"json\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .load(\"data/movies\") \\\n",
    "  .repartition(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31bab786-9937-4492-8424-c96285e59e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - what's wrong with a SinglePartition?\n",
    "# - how to add column with row_num() and count()?\n",
    "# read.parquet.count use schema\n",
    "\n",
    "whole_dataset = Window \\\n",
    "  .partitionBy() \\\n",
    "  .orderBy(col(\"Title\").asc_nulls_last())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be7edec7-4153-40a7-991b-4b623d89105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_part_df = movies_df.select(\n",
    "    col(\"Title\"),\n",
    "    row_number()\n",
    "      .over(whole_dataset)\n",
    "      .alias(\"row_num\")\n",
    ")\n",
    "single_part_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42a2a84f-31eb-4870-a829-1472ae9f4ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_part_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "970f09ac-1bf0-463d-a8af-a00b6d882ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_single_part_df = movies_df \\\n",
    "  .select(\n",
    "    col(\"Title\"),\n",
    "    monotonically_increasing_id().alias(\"row_num\")\n",
    "  )\n",
    "non_single_part_df.explain()\n",
    "# single_part_df.sample(0.1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0aa8f991-a7bc-4fe3-b267-dd9470c48d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# первая партиция стартует с 0\n",
    "# вторая с 8589934592 или 1<<33\n",
    "# все последующие через равные промежутки через 1<<33\n",
    "non_single_part_df.show(17, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "79e3fa1e-e087-44d5-94b2-db93bce29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "# How to read all data from cache?\n",
    "# Partial caching - cashing only parts which were calculated by some action. That is the cause that part of data\n",
    "# was from cache the other from source.\n",
    "\n",
    "partition_of_100_df = spark.range(0, 10000, 1, 100) \\\n",
    "  .cache() # == .persist(StorageLevel.MEMORY_AND_DISK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "149b6494-0e11-48dd-9ee7-baa64b5327f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only one partition, use only one partition FRACTION CACHE 1% - http://localhost:4040/storage/\n",
    "# consistence can be uncorrected USE .count to put all data to cache\n",
    "# deserialized - as Java object, serialized - as Array[Byte]\n",
    "\n",
    "partition_of_100_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9eb7a5a0-b016-4f5b-b005-39f032583116",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_of_100_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eae1cc66-119e-4e53-a7c9-e4d381447d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show data on local disk and disk spil\n",
    "# InMemoryRelation - load data to cache\n",
    "\n",
    "partition_of_100_df.explain()\n",
    "# InMemoryTableScn - load data to cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7e34f5-99f8-4d8d-935f-c2764f5bd92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Coalesce vs repartition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b0d254-563e-4d1f-882a-963c4a25df4b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20ac63f2-633e-41a5-9c41-3e312c3b3d4d",
   "metadata": {},
   "source": [
    "# 4 Join optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7251e0e5-3274-46c0-8b24-16f9cced6380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe of facts\n",
    "\n",
    "crime_facts = spark \\\n",
    "    .read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"data/crimes/crime.csv\")\n",
    "\n",
    "crime_facts \\\n",
    "  .cache() \\\n",
    "  .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cde0c5f4-7309-4b62-b584-560ad4f39e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catalyst optimiser\n",
    "\n",
    "# Передвинуть фильтр ближе к источнику данных\n",
    "grouped_crime_df = crime_facts \\\n",
    "  .groupBy(col(\"OFFENSE_CODE\")) \\\n",
    "  .count() \\\n",
    "  .filter(col(\"OFFENSE_CODE\") == 1402)\n",
    "\n",
    "grouped_crime_df.explain(True)\n",
    "grouped_crime_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c518f26c-a519-4491-b78a-7eb7cc833f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Небольшая таблица измерений\n",
    "\n",
    "offense_сodes = spark.\\\n",
    "    read.\\\n",
    "    option(\"header\", \"true\").\\\n",
    "    option(\"inferSchema\", \"true\").\\\n",
    "    csv(\"data/crimes/offense_codes.csv\")\n",
    "\n",
    "offense_сodes.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9f6c6542-8f57-45c3-ab84-313abdbd8f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort merge join по умолчанию\n",
    "rob_sort_merge_df = crime_facts.\\\n",
    "    join(offense_сodes, col(\"CODE\") == col(\"OFFENSE_CODE\")).\\\n",
    "    filter(col(\"NAME\").startswith(\"ROBBERY\")).\\\n",
    "    groupBy(col(\"NAME\")).\\\n",
    "    count().\\\n",
    "    orderBy(col(\"count\").desc())\n",
    "\n",
    "rob_sort_merge_df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f6d54806-d36e-4074-94d5-5977f787ee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rob_sort_merge_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2dd4b50a-467d-43ff-a2c9-3bebd1ac5380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравним с Broadcast Join\n",
    "\n",
    "rob_broadcast_df = crime_facts.\\\n",
    "    join(broadcast(offense_сodes), col(\"CODE\") == col(\"OFFENSE_CODE\")).\\\n",
    "    filter(col(\"NAME\").startswith(\"ROBBERY\")).\\\n",
    "    groupBy(col(\"NAME\")).\\\n",
    "    count().\\\n",
    "    orderBy(col(\"count\").desc())\n",
    "\n",
    "rob_broadcast_df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2089fe4b-841f-4a46-9c6c-12a0bce27be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rob_broadcast_df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dd690e-b13b-4833-95b9-9edf679b9066",
   "metadata": {},
   "source": [
    "# Shared variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "89675a59-212f-4d53-b9f3-feb7bc72265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark гарантирует, что аккумуляторы будут обновлены только 1 раз внутри каждого action\n",
    "# Spark не гарантирует, что аккумуляторы будут обновлены только 1 раз внутри transformations\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# аккумуляторы заполняем на EXECUTORS, читаем на DRIVER\n",
    "accum = sc.accumulator(0)\n",
    "\n",
    "# broadcast заполняем на DRIVER, читаем на EXECUTORS\n",
    "broadcastVar = sc.broadcast([10, 20, 30])\n",
    "broadcastVar.value\n",
    "\n",
    "sum = __builtins__.sum\n",
    "\n",
    "def my_mapper(x):\n",
    "    accum.add(1) # так лучше не делать\n",
    "    return x + sum(broadcastVar.value)\n",
    "\n",
    "res = sc.parallelize([1, 2, 3, 4]) \\\n",
    "  .map(my_mapper) \\\n",
    "  .foreach(lambda x: accum.add(x))\n",
    "\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c6d743-df7d-485e-854c-8cbf90a9f57b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
