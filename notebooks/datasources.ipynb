{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bc0cda5-e4ea-4c08-a0fa-fde8ff1eb3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.postgresql:postgresql:42.2.19,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1 pyspark-shell'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, from_json, date_format, to_timestamp\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    appName(\"Data Sources\"). \\\n",
    "    master(\"local\"). \\\n",
    "    getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 14)\n",
    "\n",
    "# config(\"spark.jars\", \"jars/postgresql-42.2.19.jar,jars/spark-sql-kafka-0-10_2.12-3.3.1.jar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ef9e33-dea4-4b85-9808-21a9ed1413b6",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"\n",
    "config(\"spark.python.worker.memory\", \"8g\"). \\\n",
    "config(\"spark.driver.memory\", \"8g\"). \\\n",
    "config(\"spark.executor.memory\", \"8g\"). \\\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890e9ec7-f46c-4aa0-b552-d12592a2b37e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Read/Write DataFrame with file system, HDFS, S3, FTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c32a3e7-9ded-4b4d-bc9e-a5979ee0a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df = spark.read. \\\n",
    "    format(\"json\"). \\\n",
    "    option(\"inferSchema\", \"true\"). \\\n",
    "    option(\"mode\", \"failFast\"). \\\n",
    "    option(\"path\", \"data/cars\"). \\\n",
    "    load()\n",
    "\n",
    "cars_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "316dbcf1-06be-4c56-bafd-687164658857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDFS\n",
    "# option(\"path\", \"hdfs://nn1home:8020/sources/cars\"). \\\n",
    "\n",
    "# FTP\n",
    "# option(\"path\", \"ftp://user:pwd/192.168.1.5/sources/cars\"). \\\n",
    "\n",
    "# S3\n",
    "# option(\"path\", s3://bucket-name/sources/cars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "460bb422-ca13-4c90-9d85-bd01f629222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df_v2 = spark.read. \\\n",
    "    format(\"json\"). \\\n",
    "    options(mode=\"failFast\", path=\"data/cars\", inferSchema=\"true\"). \\\n",
    "    load()\n",
    "\n",
    "cars_df_v2.show()\n",
    "\n",
    "# /sources/cars\n",
    "# 10.1.1.1 node1 -> block1     S3 NETWORK                             -> partition1 -> task1\n",
    "# 10.1.1.2 node2 -> block2 -> Spark Driver -> Name Node -> ip adress -> partition2 -> task2\n",
    "# 10.1.1.3 node3 -> block2                                           -> parttion3 -> task3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "749a63f5-e4f6-409c-b073-c6b3f3f7558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db481821-6c8e-4d6f-8c20-90a98e3368a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df \\\n",
    "  .repartition(3) \\\n",
    "  .write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"compression\", \"snappy\") \\\n",
    "  .parquet(\"../sources/parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ec651f7-cd44-47ca-b65a-c5239060fed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l ../sources/parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18958bbf-3484-4edd-b8e8-0cfb90360119",
   "metadata": {},
   "source": [
    "## Round Robin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6838a237-c72f-4c64-b260-74f71411f41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df \\\n",
    "  .repartition(3) \\\n",
    "  .write \\\n",
    "  .partitionBy(\"Year\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .format(\"csv\") \\\n",
    "  .save('../sources/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34426c1f-d7a4-46ed-8c42-0d232d621c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l ../sources/csv/Year\\=1973-01-01/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63f62d3d-bcbe-4b46-aefb-5d25cd6d14a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round Robin\n",
    "cars_df \\\n",
    "  .repartition(3) \\\n",
    "  .explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e50e601-5b8c-4838-8a0a-af76ec18fc51",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hash partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c6d350e-7e84-4073-97ce-4e20b63b17e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df \\\n",
    "  .repartition(3, \"Horsepower\") \\\n",
    "  .write \\\n",
    "  .partitionBy(\"Year\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .format(\"csv\") \\\n",
    "  .save('../sources/csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a0d7551-c622-4e53-af67-d0b967dfd6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l ../sources/csv/Year\\=1973-01-01/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bec84d7-9ff4-4520-a017-c6f8380b44ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hash Paritioning\n",
    "cars_df \\\n",
    "  .repartition(3, \"Horsepower\") \\\n",
    "  .explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9cdb21-1c9a-43ce-9a58-d034fd716f8f",
   "metadata": {},
   "source": [
    "## Text file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6caab552-a544-41c0-ad7c-4c2ed756c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each row is a value in a DF with a SINGLE column (\"value\")\n",
    "text_df = spark.read.text(\"data/lipsum\")\n",
    "text_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1349050c-60f1-4f2d-a94a-0f2b0af52474",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49f12a5c-b27b-4bf3-8210-783389d2d854",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df \\\n",
    "  .filter(\"length(value) > 0\") \\\n",
    "  .show(10, 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848d3860-946e-4992-a4ea-58af295baaea",
   "metadata": {},
   "source": [
    "## Hadoop HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccaef81-cdca-4846-94a5-5198ac50807b",
   "metadata": {},
   "source": [
    "### Writing to hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "deef82ad-9dad-4a72-81e9-0d7a001f1535",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df \\\n",
    "  .write \\\n",
    "  .partitionBy(\"Year\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .format(\"csv\") \\\n",
    "  .save(\"hdfs://hadoop:9000/user/hdfs/cars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414ff113-6be4-49e1-afd2-b7514b7f5068",
   "metadata": {},
   "source": [
    "### Reading From HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ada1b11d-99f5-4728-bd64-7e7d51afb3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark \\\n",
    "  .read \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"mode\", \"failFast\") \\\n",
    "  .option(\"path\", \"hdfs://hadoop:9000/user/hdfs/cars\") \\\n",
    "  .format(\"csv\") \\\n",
    "  .load() \\\n",
    "  .show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8750ec57-33b6-43c5-9ffa-e4ae46aa9aec",
   "metadata": {},
   "source": [
    "# JDBC Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd45d80f-a814-46e3-80eb-efccf8af5aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = \"org.postgresql.Driver\"\n",
    "url = \"jdbc:postgresql://postgres:5432/spark\"\n",
    "user = \"docker\"\n",
    "password = \"docker\"\n",
    "\n",
    "DBPARAMS = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": driver\n",
    "}\n",
    "\n",
    "employees = \"public.employees\"\n",
    "employees_pruned = \"\"\"(select e.first_name, e.last_name, e.hire_date from public.employees e where e.gender = 'F') as new_emp\"\"\"\n",
    "\n",
    "df = spark.read.jdbc(url=url, table=employees, properties=DBPARAMS)\n",
    "\n",
    "print(\"Общее колличество партиций:\", df.rdd.getNumPartitions())\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6128f03-7929-43de-bcd8-d59ca74a7216",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5f3590a-2140-480e-95b5-be3cf2d6ccce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.agg(F.max(F.col(\"emp_no\")), F.min(F.col(\"emp_no\"))) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f2c7c3b-e53e-4fb2-a3e2-15d3b272e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowerBound = 10010\n",
    "# UpperBound = 499990\n",
    "\n",
    "df = spark.read.jdbc(\n",
    "    url=url,\n",
    "    table=\"public.employees\",\n",
    "    properties=DBPARAMS,\n",
    "    column=\"emp_no\", # обязательно все 4 опции или ни одной\n",
    "    lowerBound = 10010, # обязательно все 4 опции или ни одной\n",
    "    upperBound = 499990, # обязательно все 4 опции или ни одной\n",
    "    numPartitions = 10 # обязательно все 4 опции или ни одной\n",
    ")\n",
    "\n",
    "print(\"Колличество партиций:\", df.rdd.getNumPartitions())\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87d1a767-52bc-4548-b19e-39618528e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предикаты\n",
    "\n",
    "pred1 = [ \"gender = 'F'\", \"gender = 'M'\", \"gender = 'O'\"]\n",
    "\n",
    "# Внимательнее с границами!\n",
    "pred2 = [\"emp_no > 10010 and emp_no <= 50000\", \"emp_no >= 50000 and emp_no <= 100000\"]\n",
    "\n",
    "df = spark.read.jdbc(\n",
    "    url=url,\n",
    "    table=\"public.employees\",\n",
    "    properties=DBPARAMS,\n",
    "    predicates=pred1\n",
    ")\n",
    "\n",
    "print(\"Колличество партиций:\", df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6183b102-0dea-4c84-b7c0-5d0e79bf9c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "368cd4b5-0c62-4510-8570-91dbba526437",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_df = spark.read. \\\n",
    "    format(\"jdbc\"). \\\n",
    "    option(\"driver\", driver). \\\n",
    "    option(\"url\", url). \\\n",
    "    option(\"user\", user). \\\n",
    "    option(\"password\", password). \\\n",
    "    option(\"dbtable\", \"public.employees\"). \\\n",
    "    option(\"partitionColumn\", \"emp_no\"). \\\n",
    "    option(\"lowerBound\", 10010). \\\n",
    "    option(\"upperBound\", 499990). \\\n",
    "    option(\"numPartitions\", \"10\"). \\\n",
    "    load()\n",
    "\n",
    "print(\"Колличество партиций:\", employees_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a7e5c23-c446-4457-b179-47e19bf30d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "depts_prunned = \"\"\"(\n",
    "  select de.emp_no\n",
    "       , d.dept_no\n",
    "       , d.dept_name\n",
    "    from public.departments d\n",
    "    join public.dept_emp de using (dept_no)\n",
    ") as new_emp\"\"\"\n",
    "\n",
    "department_df = spark \\\n",
    "  .read \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"driver\", driver) \\\n",
    "  .option(\"url\", url) \\\n",
    "  .option(\"user\", user) \\\n",
    "  .option(\"password\", password) \\\n",
    "  .option(\"dbtable\", depts_prunned) \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68dd091b-9ee4-4234-a4af-2fec2d779598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# department_df = F.broadcast(department_df)\n",
    "\n",
    "emp_dept_df = employees_df \\\n",
    "  .join(department_df, employees_df.emp_no == department_df.emp_no,  \"inner\") \\\n",
    "  .select(employees_df.emp_no, employees_df.first_name, employees_df.last_name, department_df.dept_name, department_df.dept_no) \\\n",
    "  .groupBy(\"dept_no\") \\\n",
    "  .count()\n",
    "\n",
    "print(\"Колличество партиций:\", emp_dept_df.rdd.getNumPartitions())\n",
    "\n",
    "emp_dept_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17455bea-1e97-4cca-9670-f371acbe9370",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_dept_df.show()\n",
    "\n",
    "emp_dept_df \\\n",
    "  .write \\\n",
    "  .bucketBy(4, \"dept_no\") \\\n",
    "  .sortBy(\"count\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"path\", \"hdfs://hadoop:9000/user/hdfs/spark-warehouse\") \\\n",
    "  .saveAsTable(\"employee_bucketed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c251255-ac25-440c-ae93-0cea77bd6f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark \\\n",
    "  .sql(\"SELECT * FROM employee_bucketed order by count desc\") \\\n",
    "  .show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830a46c8-13c7-4aaa-bd48-2aca1dab4f1b",
   "metadata": {},
   "source": [
    "# Kafka Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b7fba04-832f-4387-a42a-85fe32edcd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "kafka-topics.sh --bootstrap-server localhost:9092 --topic my-pyspark-topic --create --partitions 3 --replication-factor 1\n",
    "\"\"\"\n",
    "\n",
    "kafka_df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"subscribe\", \"my-pyspark-topic\") \\\n",
    "  .load()\n",
    "\n",
    "print(\"Is kafka_df a sream based DataFrame? \", kafka_df.isStreaming)\n",
    "kafka_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f33b5169-8eae-43ad-9771-636804e0271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "kafka-console-producer.sh --bootstrap-server localhost:9092 --topic my-pyspark-topic\n",
    "\"\"\"\n",
    "\n",
    "my_pyspark_topic = kafka_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"csv\") \\\n",
    "  .option(\"path\", \"../out/kafka/my-pyspark-message\") \\\n",
    "  .option(\"checkpointLocation\", \"../out/kafka/checkpoint\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f55b613c-38c6-4f1d-afbf-212f9ff34ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pyspark_topic.awaitTermination(timeout=5)\n",
    "my_pyspark_topic.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c46a91-41a2-49da-aa9b-35c110017aa7",
   "metadata": {},
   "source": [
    "Exercise: read the movies DF, then write it as\n",
    "- tab-separated \"CSV\"\n",
    "- parquet\n",
    "- table \"public.movies\" in the Postgres DB\n",
    "\n",
    "Exercise #2: find a way to read the people-1m dataFrame. Then write it as JSON."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
