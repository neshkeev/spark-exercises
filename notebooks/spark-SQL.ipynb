{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7879a40d-3ea5-4596-b5f5-f5e3f9921f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.postgresql:postgresql:42.2.19,io.delta:delta-core_2.12:2.1.0 pyspark-shell'\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "  .builder \\\n",
    "  .appName(\"Spark SQL\") \\\n",
    "  .master(\"local\") \\\n",
    "  .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "  .config(\"spark.sql.warehouse.dir\", \"../spark-warehouse\") \\\n",
    "  .enableHiveSupport() \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696bb2fa-9ac6-4427-96d4-e038abb9f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df = spark \\\n",
    "  .read \\\n",
    "  .json(\"data/cars\") \\\n",
    "  .persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "cars_df.show(10, 30) # показать 10 строк, обрубать длинные строки по 30 символу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b3de4d-bc6f-44aa-9967-1888de201ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark DSL\n",
    "american_cars_df = cars_df \\\n",
    "  .filter(col(\"Origin\") == \"Japan\") \\\n",
    "  .select(col(\"Name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "965cdb6e-5209-42f2-8e9d-022ad0132709",
   "metadata": {},
   "outputs": [],
   "source": [
    "american_cars_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be82357d-8f8d-44d6-a69a-719a960d2e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "american_cars_df.show(10, False) # показать 10 строк, не обрубать длинные строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "131f2eb6-f9c6-4b5b-b81f-82d8df61061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранить как таблицу в Spark, но не сохранять данные на диск\n",
    "#  DataFrame => SQL metastore\n",
    "print(\"DataFrame => SQL metastore = EXTERNAL TABLE\")\n",
    "cars_df.createOrReplaceTempView(\"cars\")\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0085418a-83af-49a9-8923-e6f66bd068a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark SQL != Spark DSL\n",
    "# Выполнить SQL запросы к DF, которые известны Apache Spark под какими-то именами\n",
    "american_cars_df_v2 = spark.sql(\"SELECT Name FROM cars WHERE Origin = 'Japan'\")\n",
    "american_cars_df_v2.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bacaf955-0b74-490d-ba0d-aa79e00125d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Временные таблицы действуют только на время жизни сессии\n",
    "try:\n",
    "  spark.newSession().sql(\"SELECT Name FROM cars WHERE Origin = 'Japan'\")\n",
    "except Exception as e:\n",
    "  print (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75d8dd6c-2164-4aad-8437-601bd037c8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление внешней таблицы (External Table) не удаляет данные на диске, только в metastore\n",
    "spark.sql(\"DROP TABLE cars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a39c4033-e7ca-45ea-a33f-f4289592adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ожидаемо падает:\")\n",
    "try:\n",
    "  spark.sql(\"SELECT Name FROM cars WHERE Origin = 'Japan'\")\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b470c4f3-bba4-4a25-be2a-79daabbc0107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Но данные по прежнему доступны для чтения с диска\n",
    "cars_df_again = spark.read.json(\"data/cars\")\n",
    "cars_df_again.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05cb3c6-fc40-4830-a546-f2c58446f845",
   "metadata": {},
   "source": [
    "Сохранить датафрейм как Spark таблицу: `DataFrame => SQL metastore + Spark storage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b739f79b-af84-40f0-a122-5d7436c3bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find ~ -type d -name cars_managed_table | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b144541-341b-4dec-8ca6-91a08f4e6f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df \\\n",
    "  .write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .saveAsTable(\"cars_managed_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f49ac388-cb79-482d-b768-0e20b77ee65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find ~ -type d -name cars_managed_table -exec find {} \\;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9538ee6b-99d7-4639-8660-531ccac9509c",
   "metadata": {},
   "source": [
    "`saveAsTable` выполняет действия отличные от `save()` + `orc(...)` или `parquet(....)`.\n",
    "\n",
    "В случае, например, `parquet()` указывается место для хранения файлов на диске:\n",
    "```python\n",
    "df.write \\\n",
    "  .parquet(\"data/parquet\"). \\\n",
    "  save()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c541eb9-407e-4bf2-8f55-7877183d4dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Чтение из управляемой таблицы (Managed Table) при помощи SQL запроса:\")\n",
    "american_cars_df_v2 = spark.sql(\"SELECT * FROM cars_managed_table\")\n",
    "american_cars_df_v2.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb1303b3-36af-479d-9423-8beb40d1211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(cars_df.count() == american_cars_df_v2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7eed6af-8214-46f2-9f59-a3529253fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.table == spark.read.table\n",
    "# cars_managed_df = spark.read.table(\"cars_managed_table\")\n",
    "\n",
    "print(\"Чтение из управляемой таблицы (Managed Table) при помощи Spark DSL:\")\n",
    "\n",
    "cars_managed_df = spark.table(\"cars_managed_table\")\n",
    "assert (cars_managed_df.count() != 0)\n",
    "cars_managed_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "234b702b-a0b5-4c78-b5b8-51fa03e66191",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Удалить управляемую таблицу (Managed Table)\")\n",
    "spark.sql(\"DROP TABLE cars_managed_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c807170a-2019-4d7a-803e-584cca6a7624",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find ~ -type d -name cars_managed_table | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac4c37ad-2900-48e3-adbb-027ef861df3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Через Spark SQL можно создавать таблицы и вставлять записи\n",
    "spark.sql(\"CREATE SCHEMA test\")\n",
    "spark.sql(\"CREATE TABLE test.students (name VARCHAR(64), address VARCHAR(64)) USING PARQUET PARTITIONED BY (student_id INT)\")\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO test.students\n",
    "VALUES ('Bob Brown', '456 Taylor St, Cupertino', 222222)\n",
    "     , ('Cathy Johnson', '789 Race Ave, Palo Alto', 333333)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cfece9f-8763-4f87-a2cc-931eeb957123",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl_demo_df = spark.sql(\"SELECT * FROM test.students\")\n",
    "ddl_demo_df.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebacfc89-2c21-4f76-9a1c-111b867e5fcf",
   "metadata": {},
   "source": [
    "За дополнительными сведениями об INSERT можно обратиться к [документации](https://spark.apache.org/docs/latest/sql-ref-syntax-dml-insert-table.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03b5c2c0-9367-4f9f-9293-3de77420db77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Удалить схему test\")\n",
    "spark.sql(\"DROP SCHEMA test CASCADE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85043d70-c9e6-487f-9878-e1e0622d8bb2",
   "metadata": {},
   "source": [
    "## Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b08b51-6dcf-477e-aa67-ffa698963972",
   "metadata": {},
   "source": [
    "[Delta](https://delta.io/learn/getting-started/) - современный формат эффективного хранения частоменяющихся данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65d45f17-002d-4742-b44d-ba69fceb1178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранить первую версию\n",
    "cars_df \\\n",
    "  .limit(5) \\\n",
    "  .write \\\n",
    "  .format(\"delta\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save(\"../out/my_delta_cars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c36ef013-80c6-469f-a057-4a70322b56bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Прочитать текущую версию\n",
    "delta_df = spark.read \\\n",
    "  .format(\"delta\") \\\n",
    "  .load(\"../out/my_delta_cars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "615fb4fd-2622-4c64-b71e-344f8981e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_df.createOrReplaceTempView(\"my_delta_cars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10bdf066-ddcf-4a77-bb62-3a722e904aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from my_delta_cars\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c26c48bf-bdf6-471f-ad8f-d24e4438a5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# дописать одну строку / записать новую версию\n",
    "cars_df.limit(1) \\\n",
    "    .write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(\"../out/my_delta_cars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e824ff83-9de3-4e26-9f05-ac159f165090",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from my_delta_cars\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f4ac062-d41a-46ce-8184-247818119814",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "  .read \\\n",
    "  .format(\"delta\") \\\n",
    "  .option(\"versionAsOf\", 0) \\\n",
    "  .load(\"../out/my_delta_cars\")\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2786d4-7c42-4f28-b830-23f99e338dc8",
   "metadata": {},
   "source": [
    "# Задания\n",
    "\n",
    "1. Получить список всех сотрудников и их максимальные зарплаты\n",
    "1. Получить список всех сотрудников, кто никогда не был менеджером\n",
    "1. Для каждого сотрудника, найти разницу между их зарплатой (текущей/последней) и максимальной зарплатой в их отделе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba23e3f7-ed7c-4488-bcb4-4d6b364f6eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = \"org.postgresql.Driver\"\n",
    "url = \"jdbc:postgresql://postgres:5432/spark\"\n",
    "user = \"docker\"\n",
    "password = \"docker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc6df03e-56c0-4f16-923d-647471173767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_table(table_name):\n",
    "    return spark.read. \\\n",
    "        format(\"jdbc\"). \\\n",
    "        option(\"driver\", driver). \\\n",
    "        option(\"url\", url). \\\n",
    "        option(\"user\", user). \\\n",
    "        option(\"password\", password). \\\n",
    "        option(\"dbtable\", \"public.\" + table_name). \\\n",
    "        load()\n",
    "\n",
    "employees_df = read_table(\"employees\")\n",
    "salaries_df = read_table(\"salaries\")\n",
    "dept_managers_df = read_table(\"dept_manager\")\n",
    "dept_emp_df = read_table(\"dept_emp\")\n",
    "departments_df = read_table(\"departments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "778419e3-c1d4-4713-ac13-5815c7ae68b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save table names\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "salaries_df.createOrReplaceTempView(\"salaries\")\n",
    "dept_managers_df.createOrReplaceTempView(\"dept_manager\")\n",
    "dept_emp_df.createOrReplaceTempView(\"dept_emp\")\n",
    "departments_df.createOrReplaceTempView(\"departments\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
