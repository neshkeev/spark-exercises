{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5b0bc79-e53c-4fcc-9e95-b144ad12773e",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d046736-80eb-47fe-8caf-6ac327b2860c",
   "metadata": {},
   "source": [
    "## Запуск приложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad78f7c-69a4-44dd-a9df-0a127c8ce423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad143f9f-f53e-4010-926a-21ef7e7ebfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .appName(\"master\")\n",
    "        .master(\"local[4]\")\n",
    "        .config(\"spark.log.level\", \"WARN\")\n",
    "        .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\")\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a0a22b-6cec-473f-92dd-6746df11ac6f",
   "metadata": {},
   "source": [
    "**Примечание**: Обратите внимание, что можно указать дополнительные зависимости через параметр `spark.jars.packages`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2c72be-f057-40b7-9d6b-189c6a83699d",
   "metadata": {},
   "source": [
    "## Создание логера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e40999-2d9e-47c1-9359-20593e3d8d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger(name: str=\"PYSPARK\"):\n",
    "    return sc._jvm.org.slf4j.LoggerFactory.getLogger(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79c2849-cbb1-4294-9782-dfadcc4b6fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354717c8-afef-4d85-bffd-052a51b5b1fb",
   "metadata": {},
   "source": [
    "## Подготовка данных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b48757-8cb9-4bd9-860b-34caf2627a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf /tmp/checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5f59bf-a0ee-4bb8-8d2f-f0e855bf9413",
   "metadata": {},
   "source": [
    "### Утилитные методы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a85d0b4-0d9a-40d4-b926-867de6822b9d",
   "metadata": {},
   "source": [
    "Функция `text_generator` генерирует случайную последовательность символов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8691bf-22f3-4684-ba33-072de37e28be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "def text_generator(*args, **kwargs) -> str:\n",
    "    if args:\n",
    "        max_length = args[0]\n",
    "    elif \"max_length\" in kwargs:\n",
    "        max_length = kwargs[\"max_length\"]\n",
    "    else:\n",
    "        max_length=randrange(5, 20)\n",
    "\n",
    "    return \"\".join([chr(ord('A') + randrange(26)) for _ in range(randrange(max_length))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aa65cb-f864-418a-ad6b-d94f69cec21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99235a6-8300-4176-8056-ea1e37ccfb0b",
   "metadata": {},
   "source": [
    "Функция `date_generator` генерирует случайную дату в прошлом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00852c66-aebf-48fe-8053-45f8522c76ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "from random import randrange\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "def date_generator(*args, **kwargs) -> datetime:\n",
    "    if args:\n",
    "        seconds = args[0]\n",
    "    elif \"seconds\" in kwargs:\n",
    "        seconds = kwargs[\"seconds\"]\n",
    "    else:\n",
    "        seconds=60 * 60 * 24 * 365 * randrange(20, 50)\n",
    "\n",
    "    days = seconds / 60 / 60 / 24\n",
    "    date = datetime.now() - timedelta(days=days)\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462c9277-abb0-4ddd-be0e-28c479f3d5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(date_generator())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb8be8e-c177-4ea4-b201-3a0007481fbf",
   "metadata": {},
   "source": [
    "в будущем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774706b8-6630-4174-a958-9c69a29db13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Текущая дата: {datetime.now()}\")\n",
    "print(f\"Будущая дата: {date_generator(seconds=-randrange(60))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e76a43-2f52-4699-bd7b-245370fbecba",
   "metadata": {},
   "source": [
    "Функция `number_generator` генерирует случайное число не больше заданого:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cfff54-9eed-4f8c-9b11-1efc3868bed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "def number_generator(limit: int=10**10) -> int:\n",
    "    return randrange(limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd142641-de43-4ab1-aebc-deb9a57d42d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c0635b-573c-4835-a5b6-5eddbd692b8a",
   "metadata": {},
   "source": [
    "Функция `generate_users_range_df` генерирует набор пользователей с указанным границами идентификаторов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462de325-c869-4efd-93f0-b025718b46a8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from random import randrange\n",
    "\n",
    "@dataclass\n",
    "class User:\n",
    "    id: int\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    dob: datetime\n",
    "    gender: str\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        if args:\n",
    "            self.id = args[0]\n",
    "        elif \"id\" in kwargs:\n",
    "            self.id = kwargs[\"id\"]\n",
    "        else:\n",
    "            self.id = randrange(1000)\n",
    "\n",
    "        self.first_name = text_generator()\n",
    "        self.last_name = text_generator()\n",
    "        self.dob = date_generator()\n",
    "        self.gender = \"F\" if randrange(100) > 50 else \"M\"\n",
    "\n",
    "\n",
    "def generate_users_range_df(*args, **kwargs) -> DataFrame:\n",
    "    if args:\n",
    "        first_id = args[0]\n",
    "    elif \"first_id\" in kwargs:\n",
    "        first_id = kwargs[\"first_id\"]\n",
    "    else:\n",
    "        first_id = 0\n",
    "\n",
    "    if args[1:]:\n",
    "        last_id = args[1]\n",
    "    elif \"last_id\" in kwargs:\n",
    "        last_id = kwargs[\"last_id\"]\n",
    "    else:\n",
    "        last_id = 0\n",
    "\n",
    "    assert first_id < last_id\n",
    "\n",
    "    rows = [ asdict(User(id)) for id in range(first_id, last_id + 1) ]\n",
    "    return spark.createDataFrame(rows)\n",
    "\n",
    "users_schema = generate_users_range_df(0, 1).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416b5f56-ff31-40ab-a0ca-f86248b1d943",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_users_range_df(0, 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26869606-4982-4baf-9304-d9bc7ec68ea9",
   "metadata": {},
   "source": [
    "Функция `generate_heartrate_for_dates_df` генерирует показания датчика пульса за указанный интервал времени:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fae100e-1107-458b-9f0f-d01653bd018c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "\n",
    "@dataclass\n",
    "class HeartRate:\n",
    "    user_id: int\n",
    "    timestamp: datetime\n",
    "    value: int\n",
    "\n",
    "    def __init__(self, user_id, **kwargs):\n",
    "        self.user_id = user_id\n",
    "\n",
    "        now = datetime.now()\n",
    "        if \"start\" in kwargs:\n",
    "            start = kwargs[\"start\"]\n",
    "        else:\n",
    "            start = now - timedelta(days=5)\n",
    "        if \"end\" in kwargs:\n",
    "            end = kwargs[\"end\"]\n",
    "        else:\n",
    "            end = datetime.now()\n",
    "\n",
    "        assert start < end\n",
    "\n",
    "        end_start_seconds_diff = math.floor((end - start).total_seconds())\n",
    "        now_end_diff = math.floor((now - end).total_seconds())\n",
    "        total_seconds_to_past = now_end_diff + randrange(end_start_seconds_diff)\n",
    "\n",
    "        seconds = timedelta(seconds=total_seconds_to_past).total_seconds()\n",
    "        self.timestamp = date_generator(seconds=seconds)\n",
    "        self.value = 30 + number_generator(180)\n",
    "\n",
    "\n",
    "def generate_heartrate_for_dates_df(\n",
    "    total_users: int,\n",
    "    start: datetime,\n",
    "    end: datetime,\n",
    "    *args, **kwargs\n",
    ") -> DataFrame:\n",
    "    if args:\n",
    "        size = args[0]\n",
    "    elif \"size\" in kwargs:\n",
    "        size = kwargs[\"size\"]\n",
    "    else:\n",
    "        size = randrange(1000)\n",
    "\n",
    "    rows = [ asdict(HeartRate(randrange(total_users), start=start, end=end)) for _ in range(size) ]\n",
    "\n",
    "    return spark.createDataFrame(rows)\n",
    "\n",
    "start = datetime.now() - timedelta(days=1)\n",
    "end = datetime.now()\n",
    "heartrate_schema = generate_heartrate_for_dates_df(1, start, end, 5).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0300f2-e23a-4c65-b621-ef7256486036",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now() - timedelta(days=5)\n",
    "end = datetime.now() - timedelta(days=3)\n",
    "df = generate_heartrate_for_dates_df(1, start, end, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a0ad5c-347c-40bd-9f1c-776c4a0dc486",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7630fd69-e32e-4251-a3ed-5e03fddde5fa",
   "metadata": {},
   "source": [
    "Вне указанных временных границ данных не существует:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452b46b5-cb6c-41fa-834f-7e9e8f7e7154",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where((col(\"timestamp\") > F.lit(end)) | (col(\"timestamp\") < F.lit(start))).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2614f8e-3fc7-45ce-a789-4c75da776f35",
   "metadata": {},
   "source": [
    "### Генерация данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137b7b3d-ac17-4c67-9a4e-0793c5d20ebb",
   "metadata": {},
   "source": [
    "Сгенерировать 100 пользователей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157a3439-91d5-4ac0-9277-9dbb9da48c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = generate_users_range_df(first_id=0, last_id=100)\n",
    "users_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59be786-bffe-4dbb-9d0f-369a121c96ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    users_df\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .parquet(\"/tmp/users\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc81c23-43ad-400d-9c9e-0bda3eab4ae4",
   "metadata": {},
   "source": [
    "Сгенерировать 100 тысяч показаний датчика пульса для 100 пользователей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de4f29f-97ff-451e-aff2-500c0c5ff2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now() - timedelta(days=7)\n",
    "end = datetime.now() - timedelta(days=3)\n",
    "heartrate_df = generate_heartrate_for_dates_df(total_users=100, start=start, end=end, size=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9674fef8-eb25-4481-b730-f92fc0e8af31",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    heartrate_df\n",
    "        .repartition(10, \"user_id\")\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .parquet(\"/tmp/heartrate\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4327a1f7-4734-4523-9286-0b1d87d93938",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -l /tmp/heartrate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5825562b-3468-4ffd-8e51-522ce19a3203",
   "metadata": {},
   "source": [
    "## Файловый стриминг"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3074a52-352a-4884-a932-7ca30142ac12",
   "metadata": {},
   "source": [
    "В дальнешем необходимо будет эффективно фильтровать логи, поэтому необхдодимо запомнить во сколько начался стриминг:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b3851-267e-4d32-b031-b250042a4158",
   "metadata": {},
   "outputs": [],
   "source": [
    "! date '+%Y-%m-%dT%H:%M:%SZ' > /tmp/my_pyspark_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdda2d6-a707-434f-b1c1-7e2d655b9f2c",
   "metadata": {},
   "source": [
    "Зафиксируем в логе сообщение о подготовке к стримингу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5d8ad7-dbd7-475d-a81f-abf1946eddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.warn(\"Before Reader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116c491d-e1c9-46e9-89ba-f74a21d9c504",
   "metadata": {},
   "source": [
    "Из файлов необходимо создать входную таблицу (Input Data Table):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e6b7d1-e953-4665-9dbe-36d1dbaed0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_reader_stream = (\n",
    "    spark.readStream\n",
    "        .schema(users_schema)\n",
    "        .load(\"/tmp/users\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a7325-fbbd-4912-9818-86bbc7f3588e",
   "metadata": {},
   "source": [
    "В результате появился объект `DataFrame`, т.е. можно применить к нему методы `DataFrame API`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea348b2-1c9b-4040-9864-c61832fc27ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(users_reader_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8191f6-4fa7-4879-854e-ad45c25da132",
   "metadata": {},
   "source": [
    "Можно проверить, что датафрейм является стримом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c0b563-7fea-4246-b474-6eaa76cd298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_reader_stream.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bf375b-ffcf-44dc-b4bf-7ba4b2564842",
   "metadata": {},
   "source": [
    "Для обычных датафреймов значение `isStreaming` будет ложным:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0db1b8e-4fc4-4e48-9133-905d5a9c1d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(4).isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fdb8a6-b911-47a2-aecd-59ce42faa906",
   "metadata": {},
   "source": [
    "Датафреймы являются ленивыми, поэтому никакой стрим пока не запущен. Для запуска стрима необходимо настроить приёмник:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c73a23-6bc7-4e8d-8e72-875ce84baec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.warn(\"Before Writer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917ca3cd-d7b5-40a4-8502-4e8d51a2c6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_stream = (\n",
    "    users_reader_stream\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .format(\"console\")\n",
    "        .trigger(availableNow=True)\n",
    "        .start()\n",
    ")\n",
    "users_stream.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bde83be-03e1-4000-9cbe-629ace4e5ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.warn(\"After Writer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c065f3-e3c8-48c7-91cf-7ef9dc0101fd",
   "metadata": {},
   "source": [
    "Проанализируем логи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1788dde5-f378-45b7-a2b4-3477184db66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! source ~/.bash_aliases && \\\n",
    "docker compose logs --since $(cat /tmp/my_pyspark_timestamp) pyspark | \\\n",
    "grep 'Before Reader' -A 50 | hl 'Before\\|After'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bb8957-b643-4480-b313-3aa788258c88",
   "metadata": {},
   "source": [
    "Стрим был запущен с тригером `awailableNow`, что означает, что необходимо обработать все данные и по завершению остановить стрим, т.е. сейчас стриминг прекратил свою работу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf767ef-eebf-4d97-9efa-817b5e2ee163",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_stream.isActive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d64565-0d65-49fb-802e-1f0c9479ee67",
   "metadata": {},
   "source": [
    "Датафрейм `users_reader_stream` по прежнему доступен, а значит можно запустить этот же самый стрим ещё раз:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9373892f-8f81-4a1a-b430-b48bbe8cb970",
   "metadata": {},
   "outputs": [],
   "source": [
    "! date '+%Y-%m-%dT%H:%M:%SZ' > /tmp/my_pyspark_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d26603-c98f-403b-b395-206eb131678a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.warn(\"Second Attempt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdbc290-e3b8-4fb3-a20a-6076ef724778",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_stream = (\n",
    "    users_reader_stream\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .format(\"console\")\n",
    "        .trigger(availableNow=True)\n",
    "        .start()\n",
    ")\n",
    "users_stream.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850889ca-3967-4872-8e16-c10afe49bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! source ~/.bash_aliases && \\\n",
    "docker compose logs --since $(cat /tmp/my_pyspark_timestamp) pyspark | \\\n",
    "grep 'Second Attempt' -A 50 | hl 'Second Attempt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c5e25c-d32a-46df-bb90-25263f21a506",
   "metadata": {},
   "source": [
    "У нас получилось обработать одни и те же данные два раза. Можно запустить этот стрим еще раз, и все данные будут обработаны снова. На практике не всегда это подходит, зачастую необходимо обрабатывать данные один раз."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1075f68-b36e-4e00-aba4-ceb54b21b519",
   "metadata": {},
   "source": [
    "## Чекпоинты - Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974abd3e-645a-4b7d-9fde-b420f0f23c3a",
   "metadata": {},
   "source": [
    "### Описание"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f990377b-6fc3-4a48-8f3a-967d757614b4",
   "metadata": {},
   "source": [
    "Если обратить внимание на логи снова, то можно увидеть строчку `Temporary checkpoint location...`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65afedc-1a08-44c3-b5c5-1a5ce6edeb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! source ~/.bash_aliases && \\\n",
    "docker compose logs --since $(cat /tmp/my_pyspark_timestamp) pyspark | \\\n",
    "grep 'Temporary checkpoint location' -B 10 -A 10 | hl 'Temporary checkpoint location.*:'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99419c8-168f-454e-8a42-0ad7f9dd8407",
   "metadata": {},
   "source": [
    "По умолчанию Apache Spark защищается от повторной обработки данных только в рамках одного запуска стрима. Так, если воркер упадет, то его работу продолжит другой воркер с того самого места, где остановился упавший воркер."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc0a0c8-6b9d-4d90-82a0-1feba0cec558",
   "metadata": {},
   "source": [
    "Таким образом, Spark создает временную директорию, где сохраняет прогресс обработки входных данных в стриме. После успешного завершения директория с прогрессом удаляется."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3496311-89e7-47be-9631-4a341b0715e5",
   "metadata": {},
   "source": [
    "Директория с прогрессом обработки данных в стриме называется **чекпоинтом** (**checkpoint**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f07e216-0dc6-40a5-80f9-37cada669547",
   "metadata": {},
   "source": [
    "### Конфигурация чекпоинтов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038c4d47-b9f2-4d89-9970-765dcd7d374e",
   "metadata": {},
   "source": [
    "Как было отмечено выше, чекпоинты играют важную роль в достижении нажедности стримовой обработки данных, поэтому очень важно нажедно их сохранять."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa9762-b79c-4872-9dc7-bf7310d4c4a9",
   "metadata": {},
   "source": [
    "Наилучшим решением будет использование внешнего хранилища HDFS или S3 для целей хранения чекпоинтов. Так любой воркер может продолжить работу любого другого воркера при необходимости, т.к директория с чекпоинтами надежно защищена."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35b8e19-d745-4fcd-b830-89607cb1ba48",
   "metadata": {},
   "source": [
    "Сделаем временную отметку перед стартом стрима:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545830b6-321a-4054-8402-620964a1109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! date '+%Y-%m-%dT%H:%M:%SZ' > /tmp/my_pyspark_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aaabd9-4cef-4670-ac1b-5c9ec2473833",
   "metadata": {},
   "source": [
    "Залогируем сообщение перед запуском стрима:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c04d4c-3138-480c-a345-18805870ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.warn(\"Stream with Checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0053b8c1-1d91-4164-aae0-60bdba6719be",
   "metadata": {},
   "source": [
    "Конфигурация директории для чекпоинтов осуществляется при помощи опции `checkpointLocation`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8be4f9-adc5-47c1-bcca-57f6033eb3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_stream = (\n",
    "    users_reader_stream\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .format(\"console\")\n",
    "        .option(\"checkpointLocation\", \"/tmp/checkpoint/users_file_stream\")\n",
    "        .trigger(availableNow=True)\n",
    "        .start()\n",
    ")\n",
    "users_stream.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72894f0-1c8d-4717-b9b2-7166e31428ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "! source ~/.bash_aliases && \\\n",
    "docker compose logs --since $(cat /tmp/my_pyspark_timestamp) pyspark | \\\n",
    "grep 'Stream with Checkpoint' -A 50 | hl 'Stream with Checkpoint'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd340a0e-9a95-4d77-a32c-35d4c1286149",
   "metadata": {},
   "source": [
    "Ожидаемо данные появились в консоли."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25643294-215c-4cb1-aaf4-604143e46a55",
   "metadata": {},
   "source": [
    "Последующие запуски не будут иметь эффекта:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb07ca-0c8b-4b56-90c2-d5a2b814357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! date '+%Y-%m-%dT%H:%M:%SZ' > /tmp/my_pyspark_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37bcda3-49f2-45a2-9dd1-89f33ac45d30",
   "metadata": {},
   "source": [
    "Залогируем сообщение перед запуском стрима:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada1f437-7069-4b73-ab4c-5163fb174bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.warn(\"Stream with Checkpoint After Successful Processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64799314-79b9-4a54-8e17-f77d1fd20a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_stream = (\n",
    "    users_reader_stream\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .format(\"console\")\n",
    "        .option(\"checkpointLocation\", \"/tmp/checkpoint/users_file_stream\")\n",
    "        .trigger(availableNow=True)\n",
    "        .start()\n",
    ")\n",
    "users_stream.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e737c25e-d77a-41cc-a271-16903f4f890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! source ~/.bash_aliases && \\\n",
    "docker compose logs --since $(cat /tmp/my_pyspark_timestamp) pyspark | \\\n",
    "grep 'Stream with Checkpoint After Successful Processing' -A 50 | hl 'Stream with Checkpoint After Successful Processing'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2cf59e-4cf2-49bc-8dcd-c2a2d0813a01",
   "metadata": {},
   "source": [
    "Оба запуска `writeStream` использовали одну и ту же опцию `.option(\"checkpointLocation\", \"/tmp/checkpoint/users_file_stream\")`, поэтому Spark обработал данные только один раз."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bdd08d-e3b1-40a9-99e9-541b49672033",
   "metadata": {},
   "source": [
    "Содержимое чекпоинт директории:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246447a8-215d-4521-a88c-e74e3047e179",
   "metadata": {},
   "outputs": [],
   "source": [
    "! find /tmp/checkpoint/users_file_stream -type f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24f5b80-718e-4430-9e6c-bdeacf2943c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /tmp/checkpoint/users_file_stream/sources/0/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0de4bc9-d736-497d-8059-a5be6f0101b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /tmp/checkpoint/users_file_stream/offsets/0 | head -n 2 | tail -n 1 | json_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d079f8c-1e41-4c8a-b484-dbe9a31e50eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /tmp/checkpoint/users_file_stream/metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6953d354-e52e-4db8-9143-1347f357b36b",
   "metadata": {},
   "source": [
    "Каждый стрим должен иметь свою чекпоинт директорию:\n",
    "\n",
    "> **Нельзя использовать одну чекпоинт директорию для всех стримов.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9757a417-8a1f-4379-9e2c-c06d21857aa0",
   "metadata": {},
   "source": [
    "При этом один и тот же стрим можно запускать сколько угодно раз, чекпоинт директория позволит достичь Exactly Once семантики при обработке данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dbdeff-1ee7-4ebd-8649-fbdae8163335",
   "metadata": {},
   "source": [
    "## Преобразование входных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b1fed8-877a-4dae-b1aa-af5611a48936",
   "metadata": {},
   "source": [
    "Входные данные образуют входную таблицу (Input Table), над которой можно выполнять запросы как при помощи DataFrame API так и при помощи Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032e7650-db81-4a1e-b6b9-923931559a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "heartrate_input_table = (\n",
    "    spark\n",
    "        .readStream\n",
    "        .schema(heartrate_schema)\n",
    "        .option(\"maxFilesPerTrigger\", 1)\n",
    "        .parquet(\"/tmp/heartrate\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2f6763-775d-420e-ab75-d4c9ee0db9f3",
   "metadata": {},
   "source": [
    "Входные данные могут поступать бесконечно, а значит таблица `heartrate_input_table` является неограниченной, поэтому ее обработка имеет свои особенности."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644f7438-29c4-419a-a9a4-98994136e4b0",
   "metadata": {},
   "source": [
    "Преобразование входной таблицы (Input Table) создает результирующую таблицу (Result Table). Например, можно вести подсчет числа метрик, полученных каждым пользователем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6af8f63-8c44-4b85-892d-7766c424ca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "heartrate_result_table_df = (\n",
    "    heartrate_input_table\n",
    "        .groupBy(\"user_id\")\n",
    "        .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0743c93e-04f7-4a24-b08c-bc4ef886a85a",
   "metadata": {},
   "source": [
    "До текущего момента ни одна операция не была запущена, т.к. все операции выполняются над датафреймом, который по своей природе является ленивым."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10715720-635e-4444-9c6a-c909b9e35ac5",
   "metadata": {},
   "source": [
    "Если для запуска вычислений на обычных датафреймах необходимо выполнить действие (action), то в стримовых датафреймах необходимо указать куда данные будут отправляться, т.е. указать приемик данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f2df67-86b1-4acf-acc2-9b97d2529012",
   "metadata": {},
   "source": [
    "Приемником данных могут выступать:\n",
    "\n",
    "- консоль - датафрейм выводится на консоль (не больше 20 строк за раз),\n",
    "- файл - весь датафрейм сохраняется в файле указанного формата,\n",
    "- Kafka - датафрейм отправляется в топик Apache Kafka,\n",
    "- таблица - управляемая таблица в Hive Meta Store,\n",
    "- foreachBatch - пользовательская процедура на python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6612246c-7484-4f3a-8461-83b9ecca80c2",
   "metadata": {},
   "source": [
    "Для примера выполним запись через `foreachBatch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dd1397-ca19-4de1-8b1e-06142d2efe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"drop table if exists heartrates_for_each1\")\n",
    "spark.sql(\"drop table if exists heartrates_for_each2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5569df36-6d02-4066-8ce9-34651a792e8a",
   "metadata": {},
   "source": [
    "Подготовим таблицы для хранения результатов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324d2459-7daa-4e73-ae61-742a254385f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "create table heartrates_for_each1(\n",
    "    user_id int,\n",
    "    batch_id int,\n",
    "    total int,\n",
    "    ts timestamp\n",
    ")\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "create table heartrates_for_each2\n",
    "as\n",
    "select * from heartrates_for_each1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fcbc70-551b-4c8b-9ac4-51be9ea2af54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "def my_handler(batch_df: DataFrame, batch_id: int):\n",
    "    batch_df = (\n",
    "        batch_df\n",
    "            .withColumn(\"batch_id\", F.lit(batch_id))\n",
    "            .withColumn(\"ts\", F.current_timestamp())\n",
    "            .persist(StorageLevel.MEMORY_ONLY)\n",
    "    )\n",
    "    try:\n",
    "        batch_df.write.insertInto(\"heartrates_for_each1\")\n",
    "        batch_df.write.insertInto(\"heartrates_for_each2\")\n",
    "    finally:\n",
    "        batch_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2591be-b4ee-4924-8f6e-1dd57360ee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    heartrate_result_table_df\n",
    "        .writeStream\n",
    "        .outputMode(\"complete\")\n",
    "        .foreachBatch(my_handler)\n",
    "        .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47c792b-5aa7-47d2-a52d-fe7a1bbe3e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "one = spark.table(\"heartrates_for_each1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b926971-155d-4bc4-a659-7c30072ee668",
   "metadata": {},
   "outputs": [],
   "source": [
    "two = spark.table(\"heartrates_for_each2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf40e90d-3e70-411a-8fe1-ba8b5062efe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "one.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae89e80-e60a-408a-a57f-bb3b0f6af923",
   "metadata": {},
   "outputs": [],
   "source": [
    "two.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a38acd-ac0a-41df-996b-4da993f6938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.awaitTermination(timeout=5)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16434a76-dbbc-4076-bfc0-654f47a01225",
   "metadata": {},
   "source": [
    "### О режимах работы приемника"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6d371b-194d-43b9-8a7b-bdbba519c729",
   "metadata": {},
   "source": [
    "Стриминг выше был запущен в режиме `outputMode(\"complete\")`, т.е. все данные отправлялись приёмнику, но есть и другие режимы:\n",
    "\n",
    "- `complete` - вся результирующая таблица целиком отправляется в приёмник. Приёмник сам должен решить, что делать со старыми записями\n",
    "- `append` - только новые строки отправляются в приёмник. Этот режим можно использовать только, если в результирующей таблице нет агрегатов,\n",
    "- `update` - только обновленные строки отправляются в приёмник"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a3f1c4-8168-40d7-b3be-ce17706f0173",
   "metadata": {},
   "source": [
    "В предыдущем примере приёмник работал в режиме `complete`, поэтому в итоговых таблицах есть дублирующиеся строки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c94f5dc-fbd1-4a24-9273-93bd8182acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    spark.table(\"heartrates_for_each2\")\n",
    "        .groupBy(\"user_id\").count()\n",
    "        .orderBy(col(\"count\").desc())\n",
    "        .show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a69f63c-8aa3-45de-a8db-43c38421603b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    spark.table(\"heartrates_for_each2\")\n",
    "        .select(\"user_id\", \"total\")\n",
    "        .where(\"user_id = 1\")\n",
    "        .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c57cf0-462e-42ae-b6bd-2f527693add2",
   "metadata": {},
   "source": [
    "В результирующей таблице постоянно обновляется число полученных показаний датчика для каждого пользователя, поэтому в режиме `complete` все данные отправляются в приёмник."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862f2f61-a6e5-4dd7-8bff-0df50351b1c2",
   "metadata": {},
   "source": [
    "Если выбрать режим `update`, то ситуация поменяется:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb44927e-95a1-4156-baa2-1a868521e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"truncate table heartrates_for_each1\")\n",
    "spark.sql(\"truncate table heartrates_for_each2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a8dcd0-bfef-4a47-ab3b-b39cc17eb147",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    heartrate_result_table_df\n",
    "        .writeStream\n",
    "        .outputMode(\"update\")\n",
    "        .foreachBatch(my_handler)\n",
    "        .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdebe7a-4582-4e61-a4d2-34b5855a9de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    spark.table(\"heartrates_for_each2\")\n",
    "        .groupBy(\"user_id\").count()\n",
    "        .orderBy(col(\"count\").desc())\n",
    "        .show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b81b3-8b15-4a04-8af2-9a0a1861d33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    spark.table(\"heartrates_for_each2\")\n",
    "        .select(\"user_id\", \"total\")\n",
    "        .where(\"user_id = 1\")\n",
    "        .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecc2cf9-ae2b-4cca-9ad5-66d3f8888c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.awaitTermination(timeout=5)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b027ca03-6e2b-47aa-8f43-d37a426acec9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74a512d9-ca4d-4ca8-bf3a-3fa77fb8464f",
   "metadata": {},
   "source": [
    "## Оконные операции"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1620b6-1fc9-42fb-bd86-b6ee8a18e324",
   "metadata": {},
   "source": [
    "Примеры выше вынуждали Spark поддерживать результирующую таблицу в памяти, что может быть очень затратно на больших датасетах."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f823e8-fc02-46e2-8b85-1f83c8c52edb",
   "metadata": {},
   "source": [
    "Для стримовых запросов применяют агрегации на базе окна:\n",
    "\n",
    "- открывается окно,\n",
    "- инициализируется результирующая таблица,\n",
    "- результирующая таблица вычисляет информацию на базе событий, которые появились в рамках активного окна,\n",
    "- окно закрывается,\n",
    "- результаты сбрасываются в приёмник."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0382c5-1c55-4ae2-a6a3-e2e9e201b7c7",
   "metadata": {},
   "source": [
    "### Статистика по данными Heart Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fff431-adff-424b-b76c-c52d31415fea",
   "metadata": {},
   "source": [
    "Прежде чем приступить к применению окна, необходимо понять разумные границы окна, для этого необходимо собрать статистику"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae874777-08c3-4c7a-8145-b2a945be299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0643f54-d44a-4a88-8993-c0ea46b3b3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "heartrate_df = spark.read.parquet(\"/tmp/heartrate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9764786e-fc53-4a6e-be8e-2bbe15aff2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowByUser = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1cbf2f-c387-465f-a3d4-83da1ea695d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "heartrate_diffs_df = (\n",
    "    heartrate_df\n",
    "        .withColumn(\n",
    "            \"diff\",\n",
    "             F.lead(\"timestamp\").over(windowByUser) - F.lag(\"timestamp\").over(windowByUser)\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"diff_seconds\",\n",
    "             F.lead(col(\"timestamp\").cast(\"long\")).over(windowByUser) - F.lag(col(\"timestamp\").cast(\"long\")).over(windowByUser)\n",
    "        )\n",
    ")\n",
    "\n",
    "heartrate_intervals_df = (\n",
    "    heartrate_diffs_df\n",
    "        .select(\n",
    "            F.max(\"diff\").alias(\"max_diff\"),\n",
    "            F.min(\"diff\").alias(\"min_diff\"),\n",
    "            F.mean(\"diff\").alias(\"mean_diff\"),\n",
    "            F.current_timestamp(),\n",
    "            F.current_timestamp() + F.max(\"diff\"),\n",
    "            F.current_timestamp() + F.min(\"diff\")\n",
    "        )\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6ea770-cf69-4cf4-aef8-3063e9ce7aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "heartrate_diffs_df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8c220b-960b-4139-a1a1-f3121d68c75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[max_diff, min_diff, mean_diff]] = heartrate_intervals_df.select(\"max_diff\", \"min_diff\", \"mean_diff\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa50e2cb-01ad-4eac-8aee-8e8358bde0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Максимальная разница между показаниями датчика: {max_diff}\")\n",
    "print(f\"Минимальная разница между показаниями датчика: {min_diff}\")\n",
    "print(f\"Средняя разница между показаниями датчика: {mean_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb8dbf9-5f7c-42cf-84e4-6c3799d0553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor, ceil\n",
    "\n",
    "mean_seconds_between_readings = ceil(mean_diff.total_seconds())\n",
    "max_seconds_between_readings = ceil(max_diff.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd5e3b4-1f32-4f73-ba24-b779fcda9783",
   "metadata": {},
   "outputs": [],
   "source": [
    "heartrate_input_table = (\n",
    "    spark\n",
    "        .readStream\n",
    "        .schema(heartrate_schema)\n",
    "        .option(\"maxFilesPerTrigger\", 1)\n",
    "        .parquet(\"/tmp/heartrate\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7899b5dc-b15c-44b9-84a4-3c9279431bc2",
   "metadata": {},
   "source": [
    "Входные данные могут поступать бесконечно, а значит таблица `heartrate_input_table` является неограниченной, поэтому ее обработка имеет свои особенности."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed7ecd-eeed-4dcd-87e8-999addcaaa4f",
   "metadata": {},
   "source": [
    "Преобразование входной таблицы (Input Table) создает результирующую таблицу (Result Table). Например, можно вести подсчет числа метрик, полученных каждым пользователем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a98a78-3f77-4e55-8e21-73b1142067e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "heartrate_input_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc855dd-0c44-4c26-a98a-f2c8beda2f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_heartrate_result_table_df = (\n",
    "    heartrate_input_table\n",
    "        .withWatermark(\"timestamp\", f\"{max_seconds_between_readings} minutes\")\n",
    "        .groupBy(\n",
    "            F.window(\n",
    "                \"timestamp\",\n",
    "                f\"{mean_seconds_between_readings * 10} seconds\",\n",
    "                f\"{mean_seconds_between_readings} seconds\"\n",
    "            ),\n",
    "            \"user_id\"\n",
    "        )\n",
    "        .agg(F.mean(\"value\"), F.count(\"value\"))\n",
    "        .select(\n",
    "            \"window.start\",\n",
    "            \"window.end\",\n",
    "            \"user_id\",\n",
    "            col(\"avg(value)\").alias(\"avg_value\"),\n",
    "            col(\"count(value)\").alias(\"count\")\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c0d024-6591-4514-92fe-0ad042183bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_heartrate_result_table_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8210c7a9-07b9-4575-8fd7-b84d450a3350",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf /tmp/checkpoint/windowed_heartrate_result_table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4667b364-b3e5-4b80-87f8-9acc7a563dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"drop table if exists heartrate_avg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d072c1-8f46-4c4c-8cbc-7c61521bd01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    windowed_heartrate_result_table_df\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", \"/tmp/checkpoint/windowed_heartrate_result_table_df\")\n",
    "        .toTable(\"heartrate_avg\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102994a8-5e25-4400-b6c6-fc798634c056",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(\"heartrate_avg\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c792963-86c3-48b7-88d4-7bfb34019290",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    spark.table(\"heartrate_avg\")\n",
    "        .orderBy(\"user_id\", \"start\")\n",
    "        .show(truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d386db03-cd1f-4cd2-b60e-4e0217fdc9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.awaitTermination(timeout=5)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef55c4f5-c902-4ebf-bdbf-fa64e31aef28",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4727030-436a-467c-8c0b-135112da7555",
   "metadata": {},
   "source": [
    "## Соединения стримов - Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fee4a9-7d89-4c6d-93f3-8e69b0f9038e",
   "metadata": {},
   "source": [
    "### Соединение стрима и датафрейма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b38972-edef-45d5-ab04-8b07c631de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = spark.read.parquet(\"/tmp/users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d67bc83-ebc8-4508-b939-0eaee61a7a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84c6ca3-ed2a-42a7-a3eb-bfbff84a7a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "heartrate_stream = spark.readStream.schema(heartrate_schema).parquet(\"/tmp/heartrate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4384138-19ed-48da-bc68-bd8d0314fc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "heartrate_stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96e2c62-d89d-4207-9462-cdc9c4d2351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "heartrate_stream_enriched = (\n",
    "    heartrate_stream.alias(\"h\")\n",
    "        .join(\n",
    "            users_df.alias(\"u\"),\n",
    "            col(\"h.user_id\") == col(\"u.id\")\n",
    "        )\n",
    "        .where(\"gender = 'F'\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb3267c-c4b2-4f49-97e4-c8f1b20b33e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"drop table if exists heartrate_stream_enriched_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e166838-2876-4c49-99c8-3260a31f8833",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    heartrate_stream_enriched\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", \"/tmp/checkpoint/heartrate_stream_enriched_table\")\n",
    "        .toTable(\"heartrate_stream_enriched_table\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605e8cc4-8279-4e94-b559-7c65b9b957f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from heartrate_stream_enriched_table\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a70cdde-7032-4b9e-86d5-5d2ccc4d3ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.awaitTermination(timeout=5)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f9f07d-4dc4-4718-9397-a2a9c961f99b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a26bcc51-1630-4553-be1c-4de7720084ac",
   "metadata": {},
   "source": [
    "### Соединение стрима со стримом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d57523-4f44-4a2b-ac02-21c3c012ace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "heartrate_stream = (\n",
    "    spark.readStream\n",
    "        .schema(heartrate_schema)\n",
    "        .option(\"maxFilesPerTrigger\", 1)\n",
    "        .parquet(\"/tmp/heartrate\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a75383-972d-4564-9e91-6a930621024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "heartrate_stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715655d3-dcf1-4e16-923f-069866aa11ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "heartrate_stream_watermark = heartrate_stream.withWatermark(\n",
    "    \"timestamp\",\n",
    "    f\"{max_seconds_between_readings} minutes\"\n",
    ")\n",
    "\n",
    "heartrate_stream_to_stream_enriched = (\n",
    "    heartrate_stream_watermark.alias(\"h1\")\n",
    "        .join(\n",
    "            heartrate_stream_watermark.alias(\"h2\"),\n",
    "            F.expr(\"\"\"\n",
    "                h1.user_id = h2.user_id\n",
    "            AND h1.timestamp BETWEEN h2.timestamp AND h2.timestamp + INTERVAL 1 HOUR\n",
    "            \"\"\")\n",
    "        )\n",
    "        .select(\"h1.*\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b30228-d0a4-4ffd-a201-51a056897635",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf \"/tmp/checkpoint/heartrate_stream_to_stream_enriched\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564c6a2e-cf76-4ab3-a861-8cbcd2d9e8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"drop table if exists heartrate_stream_to_stream_enriched_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e2880f-533c-4ba2-9bec-2a55488ae205",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    heartrate_stream_to_stream_enriched\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", \"/tmp/checkpoint/heartrate_stream_to_stream_enriched\")\n",
    "        .toTable(\"heartrate_stream_to_stream_enriched_table\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04d2c10-1ccb-4fb1-9823-960917ab726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from heartrate_stream_to_stream_enriched_table\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6634eb41-442a-45ef-b86f-ee4c58f32296",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.awaitTermination(timeout=5)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5822b5-7535-46c9-9c72-77ee574a9313",
   "metadata": {},
   "source": [
    "## Хранение состояния"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba15b5f-2fd7-4342-9479-2e4e9027a798",
   "metadata": {},
   "source": [
    "Во время работы стрима Spark может выполнять операции, которые обновляют состояние (`count`, `sum`, и т.д)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bd927b-905d-46cc-8a58-8d94d481dae6",
   "metadata": {},
   "source": [
    "Состояние хранится сначала в памяти, а потом периодически сбрасывается на диск. Операции с состоянием должны выполняться транзакционно, а также состояние должно версионироваться на случай внезапного сбоя."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da8a384-9cf0-4082-925e-6f3231e46bda",
   "metadata": {},
   "source": [
    "В Apache Spark 3.2 появилась реализация хранения состояния в [RocksDB](https://rocksdb.org/) - встраиваемой базе данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da44b653-a676-4578-81e2-916660fdff08",
   "metadata": {},
   "source": [
    "Перенос состояния в RocksDB позволяет сборщику мусора перестать обрабатывать структуры состояния, что снижает время на сборку мусора."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a76c9d-115b-4a4a-858b-df66a25d0b31",
   "metadata": {},
   "source": [
    "Для активации RocksDB необходимо установить опцию `spark.sql.streaming.stateStore.providerClass`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052d6cbd-28d1-4b55-92a2-37dbf9651f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.streaming.stateStore.providerClass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e35c48-0ed4-4478-b9c3-628a540ad91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.streaming.stateStore.providerClass\", \"org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af0477d-154b-4f43-8486-b490ef7f5494",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.streaming.stateStore.providerClass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a1972a-99b2-4551-89c5-4c5e5151a8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_stream = (\n",
    "    spark.readStream\n",
    "        .schema(users_schema)\n",
    "        .option(\"maxFilesPerTrigger\", 1)\n",
    "        .parquet(\"/tmp/users\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbafec7-7080-49bd-b7fa-6b5c573f8659",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf /tmp/checkpoint/users_stream_rocksdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12679bdf-0f28-4372-bfbd-9092285300f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"drop table if exists users_stream_rocksdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6502441",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    users_stream.writeStream\n",
    "        .option(\"checkpointLocation\", \"/tmp/checkpoint/users_stream_rocksdb\")\n",
    "        .trigger(availableNow=True)\n",
    "        .toTable(\"users_stream_rocksdb\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d2248e-a4e9-47fa-8898-4bcaf8f8dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(\"users_stream_rocksdb\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03e3627-41a5-423c-ad19-aa85fb3ed0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! find /tmp/checkpoint/users_stream_rocksdb -type f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8491839-3b2c-43ca-bd6e-1c6912c8f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /tmp/checkpoint/users_stream_rocksdb/offsets/0 | sed -n '2p' | json_pp | hl '.*RocksDB.*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a0777",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.awaitTermination(timeout=5)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11f99c8-f7c8-4069-ace1-1745da44cfe1",
   "metadata": {},
   "source": [
    "## Триггеры"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb74f86-10c7-4dc7-bbdc-b4b308e53e11",
   "metadata": {},
   "source": [
    "Вся работа в Structured Streaming выполняется при помощи микро-батчей:\n",
    "\n",
    "1. из источника забирается часть данных,\n",
    "1. полученные данные обрабатываются,\n",
    "1. новые данные забираются из источника,\n",
    "1. и т.д.\n",
    "\n",
    "Триггер определяет как, когда и при каких условиях микро-батчи будут запускаться. Apache Spark позволяет подобрать нужный триггер под задачу:\n",
    "\n",
    "- *(default)* - микро-батчи идут один за другим без перерыва,\n",
    "- фиксированный интервал времени,\n",
    "- только доступные данные - обработать все имеющиеся данные и остановиться,\n",
    "- непрерывный, но с фиксированным интервалом для чекпоинтов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0372d6-b971-4b05-8c0f-4c205a57f047",
   "metadata": {},
   "source": [
    "Необходимый триггер настраивается у объекта [`DataStreamWriter#trigger`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.trigger.html):\n",
    "\n",
    "```python\n",
    "spark.readStream \\\n",
    "    .parquet(\"file.parquet\") \\\n",
    "    .writeStream \\\n",
    "    .trigger(НУЖНЫЙ ТРИГГЕР ЗДЕСЬ) \\\n",
    "    .start()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fd3a25-36d2-40c7-8c34-68f553cfac38",
   "metadata": {},
   "source": [
    "## Apache Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7772c1-3ea9-41dc-b722-c708ef32b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! source ~/.bash_aliases && \\\n",
    "docker compose ps kafka redpanda | hl 'healthy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59867413-a213-4b87-a889-bb3dedf74e30",
   "metadata": {},
   "source": [
    "Создать топик `my-users-topic`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eed5af4-1547-4a80-a32e-5aec6268c885",
   "metadata": {},
   "outputs": [],
   "source": [
    "! source ~/.bash_aliases && HOST=kafka execute \\\n",
    "kafka-topics \\\n",
    "    --bootstrap-server kafka:9092 \\\n",
    "    --topic my-users-topic \\\n",
    "    --create \\\n",
    "    --partitions 1 \\\n",
    "    --replication-factor 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d927b7-d51e-4056-93b1-9387ec94147a",
   "metadata": {},
   "source": [
    "Проверить, что топик `my-users-topic` появился:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0fef32-95e3-4556-9200-40574b92c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "! source ~/.bash_aliases && HOST=kafka execute \\\n",
    "kafka-topics \\\n",
    "    --bootstrap-server kafka:9092 \\\n",
    "    --list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa50c5a-ef7b-4606-8cd0-15b7e9a42388",
   "metadata": {},
   "source": [
    "Чекпоинт является основным механизмом, благодаря которому Apache Spark обеспечивает `Exactly-Once` семантику обработки запросов. Но `Exactly-Once` часто может быть недопустимо медленным, поэтому на практике чаще прибегают к `At-Least-Once`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab350cc-cedf-40f3-a3d9-269e5829d43c",
   "metadata": {},
   "source": [
    "Идея в том, что фиксация чекпоинтов выполняется асинхронно, поэтому одни и те же записи могут и будут быть обработаны несколько раз, поэтому нужно быть готовым к дубликатам строк. Жертва ненажедным чекпоинтом в обмен на повышение скорости обработки часто имеет большой смысл на практике."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdf8727-2073-4ff1-bd23-bf2daa660d4f",
   "metadata": {},
   "source": [
    "Семантика At-Least-Once поддерживается только, если Apache Spark взаимодействует с Apache Kafka, для других приёмников At-Least-Once недоступно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84140e6b-4525-4c66-a64a-166d5277e8da",
   "metadata": {},
   "source": [
    "Apache Spark позволяет перейти к At-Least-Once семантике. Для этого пользователю необходимо активировать асинхронную фиксацию чекпоинтов при помощи опции `asyncProgressTrackingEnabled`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d048384-c085-499d-8086-058e52d012b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    spark.read\n",
    "        .parquet(\"/tmp/users\")\n",
    "        .select(F.struct(\"*\").alias(\"value\"))\n",
    "        .select(F.to_json(\"value\"))\n",
    "        .show(truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8935d0-ef6b-430a-a59f-72cadcf2f642",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_stream = (\n",
    "    spark.readStream\n",
    "        .schema(users_schema)\n",
    "        .parquet(\"/tmp/users\")\n",
    "        .select(\n",
    "            col(\"id\").cast(\"string\").alias(\"key\"),\n",
    "            F.struct(\"*\").alias(\"value\")\n",
    "        )\n",
    "        .select(\n",
    "            \"key\",\n",
    "            F.to_json(\"value\").alias(\"value\")\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baca5101-fd37-4c35-98dd-77bd116f7295",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac6f3a-3a1f-4f37-a061-d0f1c5f0815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf /tmp/checkpoint/kafka-async-checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312583db-6a2e-4062-b535-6384c058057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    users_stream.writeStream\n",
    "        .option(\"checkpointLocation\", \"/tmp/checkpoint/kafka-async-checkpoint\")\n",
    "        .option(\"asyncProgressTrackingEnabled\", True)\n",
    "        .option(\"asyncProgressTrackingCheckpointIntervalMs\", 5000)\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "        .option(\"topic\", \"my-users-topic\")\n",
    "        .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d6e8ce-03d4-44ae-877a-a455ee51f7a6",
   "metadata": {},
   "source": [
    "Проверить, что данные дошли до Kafka брокера:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee17237-3ae8-4ef6-bf1e-3c23b5428b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! source ~/.bash_aliases && HOST=kafka execute \\\n",
    "kafka-console-consumer \\\n",
    "    --bootstrap-server kafka:9092 \\\n",
    "    --topic my-users-topic \\\n",
    "    --from-beginning \\\n",
    "    --timeout-ms 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc92a2-178a-4c0d-a28a-be4a9b64a80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.awaitTermination(timeout=5)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e22c2-678e-4e9a-bceb-2c823fa60b9a",
   "metadata": {},
   "source": [
    "### Kafka Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a09a74-a3af-40f7-9c73-a9637c2c3f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf /tmp/checkpoint/users_from_kafka-checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567f47ff-8b4b-4b00-aff6-1eee2bdf3a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS users_from_kafka\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35d636a-88e8-4d3c-a840-d764f773e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_read_stream = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "        .format(\"kafka\")\n",
    "        .option(\"asyncProgressTrackingEnabled\", True)\n",
    "        .option(\"asyncProgressTrackingCheckpointIntervalMs\", 5000)\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "        .option(\"startingOffsets\", \"earliest\")\n",
    "        .option(\"subscribe\", \"my-users-topic\")\n",
    "        .load()\n",
    ")\n",
    "\n",
    "kafka_read_stream_clean = (\n",
    "    kafka_read_stream.select(\n",
    "        col(\"key\").cast(\"string\"),\n",
    "        col(\"value\").cast(\"string\"),\n",
    "    ).select(\n",
    "        col(\"key\").cast(\"INT\").alias(\"id\"),\n",
    "        F.from_json(\"value\", users_schema).alias(\"value\")\n",
    "    ).select(\n",
    "        \"id\",\n",
    "        \"value.first_name\",\n",
    "        \"value.last_name\",\n",
    "        \"value.gender\",\n",
    "        \"value.dob\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072371e1-a904-498a-a8d9-b8787de120e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    kafka_read_stream_clean\n",
    "        .writeStream\n",
    "        .option(\"checkpointLocation\", \"/tmp/checkpoint/users_from_kafka-checkpoint\")\n",
    "        .toTable(\"users_from_kafka\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae6bec4-1ccf-43e6-9a31-dc815375e57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from users_from_kafka\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854dc9b3-f303-47b6-b286-9de348ca01d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from users_from_kafka\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d998e318-e999-4f4e-bea6-c51c36ba05b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.awaitTermination(timeout=5)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37497de-a836-45ed-998d-41dd3d321c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
